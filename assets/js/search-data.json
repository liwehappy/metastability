{
  
    
        "post0": {
            "title": "Fake News Prediction with EmbeddingBag and LSTM(Glove word Embedding)",
            "content": "Approach text classification problem with advice from the Book &quot;Practical Natural Language Processing - A Comprehensive guide to building real-world NLP NLP system&quot; . Pipeline for building Text Classification Systems . Collect data and create its corresponding label appropriately for the dataset and task at hand. | Split dataset into train, valid, and test part of it, then decide on suitable evalation metrics. | Transform raw texts into feature vectors. | Train a classifier from the feature vectors and labels correspondingly. | Utilize the evaluation metric to highlight the model performance. | Model deployment and monitor its performance. | Baseline Models . It is almost helpful to build a simple model based on a set of heuristics or rules: . deploy viable minimum viable product(MVP) | Speed | For evaluation metric | . Experiment with many classifiers with our one pipeline . Common Pitfalls of Classification Problem . Large number of features introduce sparsity . Method: Consider reducing the number of features. . | Class imbalance . Method: . oversampling the minority class or undersampling the majority class to create balanced dataset | Imbalanced-Learn Library | | . Feature Enginering with text data - Word Embedding(Word2Vec) . Pre-trained Models: . GloVe | Genism | . Custom word embeddings is necessary if a domain knowledge whose vocabularies is distinctly different from that of pre-train embeddings. . Rule of Thumb: Pre-trained embeddings perform better if overlap between custom vocabularies and pre-trained vocabularies exceed 80%. . Caution: Pre-trained embeddings could be bulky when deploying such model into production. . Subword Embedding(&quot;character n-gram&quot; embedding) using fastText . In order to address the out-of-vocabularies(OOV) problem, sub-word level representation of an individual word is used. This approach can address words that did not show up in training data, also perform efficiently on a very large corpus(fast for setting up strong baseline). . Hightlights: . Doc2vec can also be used as a nearest neighbor classifier for multiplelabel classification problem. | Model tuning is required. | Model is bulky and not as fast as fastText as well. | . Document Embeddings . Purpose: Learn a direct representation for the entire document. . Downside: model size is large. Note: There are some options for size reduction. . Goal of this notebook: Perform text classification with pytorch deep learning library from start. . Setup . Set directory locations. If working on Google Colab: copy files and install required libraries. . from google.colab import files import os.path import torch device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) if not os.path.isfile(&#39;Fake.csv&#39;) and not os.path.isfile(&#39;True.csv&#39;): # !pip install kaggle uploaded = files.upload() # Then move kaggle.json into the folder where the API expects to find it. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset !unzip /content/fake-and-real-news-dataset . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json Downloading fake-and-real-news-dataset.zip to /content 85% 35.0M/41.0M [00:00&lt;00:00, 143MB/s] 100% 41.0M/41.0M [00:00&lt;00:00, 161MB/s] Archive: /content/fake-and-real-news-dataset.zip inflating: Fake.csv inflating: True.csv . Load Python Settings . Common imports, defaults for formatting in Matplotlib, Pandas etc. . import pandas as pd import numpy as np from sklearn import preprocessing import nltk nltk.download(&#39;opinion_lexicon&#39;) from IPython.display import display import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import confusion_matrix . [nltk_data] Downloading package opinion_lexicon to /root/nltk_data... [nltk_data] Unzipping corpora/opinion_lexicon.zip. . Introducing the Fake News Dataset(EDA) . Summay: . df_fake = pd.read_csv(&#39;Fake.csv&#39;) df_fake[&#39;authenticity&#39;] = pd.Series(np.ones(df_fake.shape[0])) print(df_fake.shape) df_true = pd.read_csv(&#39;True.csv&#39;) df_true[&#39;authenticity&#39;] = pd.Series(np.zeros(df_true.shape[0])) print(df_true.shape) df = pd.concat([df_fake, df_true], axis=0).reset_index(drop=True) df[&#39;text_length&#39;] = df[&#39;text&#39;].str.len() print(df.shape) print(df.head()) . (23481, 5) (21417, 5) (44898, 6) title ... text_length 0 Donald Trump Sends Out Embarrassing New Year’... ... 2893 1 Drunk Bragging Trump Staffer Started Russian ... ... 1898 2 Sheriff David Clarke Becomes An Internet Joke... ... 3597 3 Trump Is So Obsessed He Even Has Obama’s Name... ... 2774 4 Pope Francis Just Called Out Donald Trump Dur... ... 2346 [5 rows x 6 columns] . In authenticity column, 1 represent fake news, 0 means the news is authentic. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 44898 entries, 0 to 44897 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 title 44898 non-null object 1 text 44898 non-null object 2 subject 44898 non-null object 3 date 44898 non-null object 4 authenticity 44898 non-null float64 5 text_length 44898 non-null int64 dtypes: float64(1), int64(1), object(4) memory usage: 2.1+ MB . Indeed there are no missing values. . df.isna().sum() . title 0 text 0 subject 0 date 0 authenticity 0 text_length 0 dtype: int64 . df.subject.value_counts() . politicsNews 11272 worldnews 10145 News 9050 politics 6841 left-news 4459 Government News 1570 US_News 783 Middle-east 778 Name: subject, dtype: int64 . This dataset is not so imbalanced in terms of its label. . display(df.authenticity.value_counts()) sns.catplot(data=df, x=&#39;authenticity&#39;, kind=&#39;count&#39;, height=5, aspect=2) . 1.0 23481 0.0 21417 Name: authenticity, dtype: int64 . &lt;seaborn.axisgrid.FacetGrid at 0x7fd0671e0e10&gt; . The distribution of text length is obviusly left-skewed . df[&#39;text_length&#39;].plot(kind=&#39;box&#39;, vert=False) plt.show() df[&#39;text_length&#39;].plot(kind=&#39;hist&#39;, bins=100) plt.show() . Both &quot;US_News&quot; and &quot;Middle-east&quot; are relatively lengthy in comparison with articles from other categories. . sns.catplot(data=df, x=&#39;subject&#39;, y=&#39;text_length&#39;, kind=&#39;box&#39;, height=5, aspect=2) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd05d2a9ed0&gt; . Fake news seem to have less short articles and more outliers. On the other hand, the length of authentic news ranges from short to moderate and their outliers are much less extreme in terms of text length. . sns.catplot(data=df, x=&#39;authenticity&#39;, y=&#39;text_length&#39;, kind=&#39;violin&#39;, height=5, aspect=2) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd05d209190&gt; . Interestingly, politicsNews and worldnews categories have significant proportion of genuine news, while fake news almost comprises the rest of categories. . sns.catplot(data=df, x=&#39;authenticity&#39;, kind=&#39;count&#39;, col=&#39;subject&#39;, col_wrap=4, height=5, aspect=0.8) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd05d287950&gt; . Word Frequency Analysis . import regex as re def tokenize(text): return re.findall(r&#39;[ w-]* p{L}[ w-]*&#39;, text) nltk.download(&#39;stopwords&#39;) stopwords = set(nltk.corpus.stopwords.words(&#39;english&#39;)) def remove_stop(tokens): return [t for t in tokens if t.lower() not in stopwords] pipeline = [str.lower, tokenize, remove_stop] def prepare(text, pipeline): tokens = text for transform in pipeline: tokens = transform(tokens) return tokens . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. . df[&#39;tokens&#39;] = df[&#39;text&#39;].apply(prepare, pipeline=pipeline) df[&#39;num_tokens&#39;] = df[&#39;tokens&#39;].map(len) . from collections import Counter counter = Counter() def count_words(df, column=&#39;tokens&#39;, preprocess=None, min_freq=2): def update(doc): tokens = doc if preprocess is None else preprocess(doc) counter.update(tokens) counter = Counter() df[column].map(update) freq_df = pd.DataFrame.from_dict(counter, orient=&#39;index&#39;, columns=[&#39;freq&#39;]) freq_df = freq_df.query(&#39;freq &gt;= @min_freq&#39;) freq_df.index.name = &#39;token&#39; return freq_df.sort_values(&#39;freq&#39;, ascending=False) . freq_df_true = count_words(df.loc[df.authenticity == 0]) freq_df_fake = count_words(df.loc[df.authenticity == 1]) . ax = freq_df_fake.head(15).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=&#39;Top Words of Fake News&#39;) . [Text(0, 0.5, &#39;Token&#39;), Text(0.5, 0, &#39;Frequency&#39;), Text(0.5, 1.0, &#39;Top Words of Fake News&#39;)] . ax = freq_df_true.head(15).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=&#39;Top Words of Authentic News&#39;) . [Text(0, 0.5, &#39;Token&#39;), Text(0.5, 0, &#39;Frequency&#39;), Text(0.5, 1.0, &#39;Top Words of Authentic News&#39;)] . Top words per subject of news. . for sub in df.subject.unique(): freq_df = count_words(df.loc[df.subject == sub]) ax = freq_df.head(15).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=f&#39;Top Words of {sub} News&#39;) . N-Grams analysis . from wordcloud import WordCloud from matplotlib import pyplot as plt # define word cloud helper function to bettere format wordcloud def wordcloud(word_freq, title=None, max_words=200, stopwords=None): wc = WordCloud(width=800, height=400, background_color= &quot;black&quot;, colormap=&quot;Paired&quot;, max_font_size=150, max_words=max_words) # convert DataFrame into dict if type(word_freq) == pd.Series: counter = Counter(word_freq.fillna(0).to_dict()) else: counter = word_freq # filter stop words in frequency counter if stopwords is not None: counter = {token:freq for (token, freq) in counter.items() if token not in stopwords} wc.generate_from_frequencies(counter) plt.title(title) plt.imshow(wc, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) . Test out basic ngram function . text = &quot;the visible manifestation of the global climate change&quot; tokens = tokenize(text) tokens . [&#39;the&#39;, &#39;visible&#39;, &#39;manifestation&#39;, &#39;of&#39;, &#39;the&#39;, &#39;global&#39;, &#39;climate&#39;, &#39;change&#39;] . [(ngram) for ngram in zip(*[tokens[i:] for i in range(2)])] . [(&#39;the&#39;, &#39;visible&#39;), (&#39;visible&#39;, &#39;manifestation&#39;), (&#39;manifestation&#39;, &#39;of&#39;), (&#39;of&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;global&#39;), (&#39;global&#39;, &#39;climate&#39;), (&#39;climate&#39;, &#39;change&#39;)] . Define a function to generate ngram . def ngrams(tokens, n=2, sep=&#39; &#39;, stopwords=set()): return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)]) if len([t for t in ngram if t in stopwords]) == 0] print(&quot;Bigrams:&quot;, &#39;|&#39;.join(ngrams(tokens, 2, stopwords=stopwords))) print(&quot;Trigrams:&quot;, &#39;|&#39;.join(ngrams(tokens, 3, stopwords=stopwords))) . Bigrams: visible manifestation|global climate|climate change Trigrams: global climate change . df[&#39;bigrams&#39;] = df[&#39;text&#39;].apply(prepare, pipeline=[str.lower, tokenize]).apply(ngrams, n=2, stopwords=stopwords) . Bigram Frequency . We also plot a bigram frequency histogram per news subjects here to show the difference between subject. Note that our previous analysis have shown true news are exclusively concentracted at last two categories which are &quot;politicsNews&quot; and &quot;worldnews.&quot; . for sub in df.subject.unique(): freq_df = count_words(df.loc[df.subject == sub], &#39;bigrams&#39;) ax = freq_df.head(15).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=f&#39;Top Words of {sub} News&#39;) . Bigrams in True/Fake News . for sub in df.authenticity.unique(): freq_df = count_words(df.loc[df.authenticity == sub], &#39;bigrams&#39;) ax = freq_df.head(15).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() if sub == 1.0: ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=f&#39;Top Words of Fake News&#39;) else: ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=f&#39;Top Words of True News&#39;) . Bigram Tf-idf Vectors . Implement idf computation to accurately highlight those important words instead of highlighting those merely frequent words. . def compute_idf(df, column=&#39;tokens&#39;, preprocess=None, min_df=2): def update(doc): tokens = doc if preprocess is None else preprocess(doc) counter.update(set(tokens)) counter = Counter() df[column].map(update) idf_df = pd.DataFrame.from_dict(counter, orient=&#39;index&#39;, columns=[&#39;df&#39;]) idf_df = idf_df.query(&#39;df &gt;= @min_df&#39;).copy() #To not completely ignore terms which appear in most of corpus. idf_df[&#39;idf&#39;] = np.log(len(df) / idf_df[&#39;df&#39;]) + 0.1 idf_df.index.name = &#39;token&#39; return idf_df . Bigram Word Cloud based on tf-idf vectors . Fake News . idf_df_fake = compute_idf(df.loc[df.authenticity == 1]) idf_df_fake = pd.concat([idf_df_fake, compute_idf(df.loc[df.authenticity == 1], &#39;bigrams&#39;, min_df=10)]) freq_df_fake = count_words(df.loc[df.authenticity == 1], &#39;bigrams&#39;) freq_df_fake[&#39;tfidf&#39;] = freq_df_fake[&#39;freq&#39;] * idf_df_fake[&#39;idf&#39;] wordcloud(freq_df_fake[&#39;tfidf&#39;], title=&#39;All bigrams from Fake News&#39;, max_words=50) . ax = freq_df_fake[&#39;tfidf&#39;].sort_values(ascending=False).head(20).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=f&#39;Top Bigrams of Fake News based on tf-idf vectors&#39;) . [Text(0, 0.5, &#39;Token&#39;), Text(0.5, 0, &#39;Frequency&#39;), Text(0.5, 1.0, &#39;Top Bigrams of Fake News based on tf-idf vectors&#39;)] . Authentic News . idf_df_true = compute_idf(df.loc[df.authenticity == 0]) idf_df_true = pd.concat([idf_df_true, compute_idf(df.loc[df.authenticity == 0], &#39;bigrams&#39;, min_df=10)]) freq_df_true = count_words(df.loc[df.authenticity == 0], &#39;bigrams&#39;) freq_df_true[&#39;tfidf&#39;] = freq_df_true[&#39;freq&#39;] * idf_df_true[&#39;idf&#39;] wordcloud(freq_df_true[&#39;tfidf&#39;], title=&#39;All bigrams from True News&#39;, max_words=50) . ax = freq_df_true[&#39;tfidf&#39;].sort_values(ascending=False).head(20).plot(kind=&#39;barh&#39;, figsize=(10, 6), fontsize=20) ax.invert_yaxis() ax.set(xlabel=&#39;Frequency&#39;, ylabel=&#39;Token&#39;, title=f&#39;Top Bigrams of True News based on tf-idf vectors&#39;) . [Text(0, 0.5, &#39;Token&#39;), Text(0.5, 0, &#39;Frequency&#39;), Text(0.5, 1.0, &#39;Top Bigrams of True News based on tf-idf vectors&#39;)] . Baseline Pytorch Model . Data Preprocessing . import re RE_SUSPICIOUS = re.compile(r&#39;[&amp;#&lt;&gt;{} [ ] ]&#39;) #Basic text impurity metric def impurity(text, min_len=10): &quot;&quot;&quot;returns the share of suspicious characters in a text&quot;&quot;&quot; if text == None or len(text) &lt; min_len: return 0 else: return len(RE_SUSPICIOUS.findall(text))/len(text) . import html # Clean Text def clean(text): # convert html escapes like &amp;amp; to characters. text = html.unescape(text) # tags like &lt;tab&gt; text = re.sub(r&#39;&lt;[^&lt;&gt;]*&gt;&#39;, &#39; &#39;, text) # markdown URLs like [Some text](https://....) text = re.sub(r&#39; [([^ [ ]]*) ] ([^ ( )]* )&#39;, r&#39; 1&#39;, text) # text or code in brackets like [0] text = re.sub(r&#39; [[^ [ ]]* ]&#39;, &#39; &#39;, text) # standalone sequences of specials, matches &amp;# but not #cool text = re.sub(r&#39;(?:^| s)[&amp;#&lt;&gt;{} [ ]+| :-]{1,}(?: s|$)&#39;, &#39; &#39;, text) # standalone sequences of hyphens like or == text = re.sub(r&#39;(?:^| s)[ -= +]{2,}(?: s|$)&#39;, &#39; &#39;, text) # sequences of white spaces text = re.sub(r&#39; s+&#39;, &#39; &#39;, text) # Character Data text = re.sub(r&#39;//.*//&#39;, &#39; &#39;, text) # Sequence of # and numbers text = re.sub(r&#39;(#+ d+)&#39;, &#39; &#39;, text) # Remove youtube address text = re.sub(r&#39;(https://www.+)&#39;, &#39; &#39;, text) return text.strip() . df[&#39;clean_text&#39;] = df[&#39;text&#39;].map(clean) df[&#39;impurity&#39;] = df[&#39;clean_text&#39;].apply(impurity, min_len=20) df[[&#39;clean_text&#39;, &#39;impurity&#39;]].sort_values(by=&#39;impurity&#39;, ascending=False).head(3) . clean_text impurity . 12737 Notify the CDC. It&#39;s spreading. #BenCarson #Mo... | 0.024000 | . 19804 Notify the CDC. It&#39;s spreading. #BenCarson #Mo... | 0.024000 | . 20216 The Queen of the DNC is in big trouble 1000+ B... | 0.019512 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; import os from torch.utils.data import Dataset, DataLoader import torch # Custom dataset for Pytorch libary class FakeNewsDataset(Dataset): def __init__(self): self.text_labels = df.authenticity self.article = df.clean_text def __len__(self): return len(df) def __getitem__(self, idx): if isinstance(idx, torch.Tensor): idx = idx.tolist() label = self.text_labels.iloc[idx] contents = self.article.iloc[idx] return (label, contents) def yield_tokens(data_iter): for text in data_iter: yield tokenizer(text) . dataset = FakeNewsDataset() . Embedding Bag Pytorch Model . Model Building . import torch from torch.utils.data import DataLoader def collate_batch(batch): &quot;&quot;&quot; Combine label, texts, offsets(for the purpose of identifying instances) per instances &quot;&quot;&quot; label_list, text_list, offsets = [], [], [0] for (_label, _text) in batch: label_list.append(label_pipeline(_label)) processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) text_list.append(processed_text) offsets.append(processed_text.size(0)) label_list = torch.tensor(label_list, dtype=torch.int64) offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) text_list = torch.cat(text_list) return label_list.to(device), text_list.to(device), offsets.to(device) . from torch import nn import torch.nn.functional as F # Implement Embedding Bag class TextClassificationModel(nn.Module): def __init__(self, vocab_size, embed_dim, num_class): super(TextClassificationModel, self).__init__() self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) self.fc = nn.Linear(embed_dim, num_class) self.init_weights() def init_weights(self): initrange = 0.5 self.embedding.weight.data.uniform_(-initrange, initrange) self.fc.weight.data.uniform_(-initrange, initrange) self.fc.bias.data.zero_() def forward(self, text, offsets): embedded = self.embedding(text, offsets) # out = F.sigmoid(self.fc(embedded)) return self.fc(embedded) # return out . import time from pdb import set_trace from sklearn.metrics import confusion_matrix def train(dataloader): model.train() total_acc, total_count = 0, 0 log_interval = 250 start_time = time.time() for idx, (label, text, offsets) in enumerate(dataloader): optimizer.zero_grad() predicted_label = model(text, offsets) # loss = criterion(predicted_label, label) loss = criterion(predicted_label, label.unsqueeze(1).type(torch.FloatTensor)) loss.backward() #Gradient clipping by norm (Addressing potential problem of exploding/vanishing gradients) torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) optimizer.step() predicted_label = (predicted_label &gt;= 0).float() total_acc += (torch.squeeze(predicted_label) == label).sum().item() total_count += label.size(0) #report accuracy per 250 batches if idx % log_interval == 0 and idx &gt; 0: elapsed = time.time() - start_time # print(&#39;| epoch {:3d} | {:5d}/{:5d} batches &#39; &#39;| accuracy {:8.3f}&#39;.format(epoch, idx, len(dataloader), total_acc/total_count)) total_acc, total_count = 0, 0 start_time = time.time() def evaluate(dataloader, confu_matri=False): model.eval() total_acc, total_count = 0, 0 with torch.no_grad(): y_pred = [] y_test = [] for idx, (label, text, offsets) in enumerate(dataloader): predicted_label = model(text, offsets) loss = criterion(predicted_label, label.unsqueeze(1).type(torch.FloatTensor)) predicted_label = (predicted_label &gt;= 0).float() y_pred.append(predicted_label) y_test.append(label) total_acc += (torch.squeeze(predicted_label) == label).sum().item() total_count += label.size(0) if confu_matri == True: y_pred = torch.cat(y_pred, dim=0) y_test = torch.cat(y_test, dim=0) sns.heatmap(confusion_matrix(y_test, y_pred), annot=True) plt.show() return total_acc/total_count . from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator tokenizer = get_tokenizer(&#39;basic_english&#39;) train_size = int(0.8 * len(dataset)) test_size = len(df) - train_size train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size]) vocab = build_vocab_from_iterator(yield_tokens(dataset[train_dataset.indices][1]), specials=[&quot;&lt;unk&gt;&quot;]) # vocab = build_vocab_from_iterator(yield_tokens(df.text), specials=[&quot;&lt;unk&gt;&quot;]) #make default index name the same as index of &lt;unk&gt; token vocab.set_default_index(vocab[&quot;&lt;unk&gt;&quot;]) . num_class = 1 vocab_size = len(vocab) emsize = 64 model = TextClassificationModel(vocab_size, emsize, num_class).to(device) . vocab([&#39;here&#39;, &#39;is&#39;, &#39;an&#39;, &#39;example&#39;, &#39;aaaaa&#39;]) . [152, 13, 34, 973, 0] . text_pipeline = lambda x: vocab(tokenizer(x)) label_pipeline = lambda x: int(x) . from torch.utils.data.dataset import random_split from torchtext.data.functional import to_map_style_dataset #Hyperparameters EPOCH = 10 LR = 0.05 # BATCH_SIZE = 64 BATCH_SIZE = 32 # criterion = torch.nn.CrossEntropyLoss() criterion = torch.nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model.parameters(), lr=LR) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) total_accu = None #make 0.95 of trainset to be train data and 0.05 of trainset to be valid_data num_train = int(len(train_dataset) * 0.8) split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train]) train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch) valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch) test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch) for epoch in range(1, EPOCH + 1): epoch_start_time = time.time() train(train_dataloader) accu_val = evaluate(valid_dataloader) if total_accu is not None and total_accu &gt; accu_val: scheduler.step() else: total_accu = accu_val print(&#39;-&#39; * 59) print(&#39;| end of epoch {:3d} | time: {:5.2f}s | &#39; &#39;valid accuracy {:8.3f} &#39;.format(epoch, time.time() - epoch_start_time, accu_val)) print(&#39;-&#39; * 59) . | epoch 1 | 250/ 898 batches | accuracy 0.536 | epoch 1 | 500/ 898 batches | accuracy 0.554 | epoch 1 | 750/ 898 batches | accuracy 0.580 -- | end of epoch 1 | time: 28.03s | valid accuracy 0.668 -- | epoch 2 | 250/ 898 batches | accuracy 0.625 | epoch 2 | 500/ 898 batches | accuracy 0.692 | epoch 2 | 750/ 898 batches | accuracy 0.658 -- | end of epoch 2 | time: 27.86s | valid accuracy 0.744 -- | epoch 3 | 250/ 898 batches | accuracy 0.801 | epoch 3 | 500/ 898 batches | accuracy 0.750 | epoch 3 | 750/ 898 batches | accuracy 0.806 -- | end of epoch 3 | time: 27.92s | valid accuracy 0.807 -- | epoch 4 | 250/ 898 batches | accuracy 0.839 | epoch 4 | 500/ 898 batches | accuracy 0.849 | epoch 4 | 750/ 898 batches | accuracy 0.877 -- | end of epoch 4 | time: 27.87s | valid accuracy 0.885 -- | epoch 5 | 250/ 898 batches | accuracy 0.888 | epoch 5 | 500/ 898 batches | accuracy 0.904 | epoch 5 | 750/ 898 batches | accuracy 0.897 -- | end of epoch 5 | time: 27.85s | valid accuracy 0.900 -- | epoch 6 | 250/ 898 batches | accuracy 0.906 | epoch 6 | 500/ 898 batches | accuracy 0.909 | epoch 6 | 750/ 898 batches | accuracy 0.918 -- | end of epoch 6 | time: 27.90s | valid accuracy 0.916 -- | epoch 7 | 250/ 898 batches | accuracy 0.925 | epoch 7 | 500/ 898 batches | accuracy 0.919 | epoch 7 | 750/ 898 batches | accuracy 0.924 -- | end of epoch 7 | time: 27.93s | valid accuracy 0.922 -- | epoch 8 | 250/ 898 batches | accuracy 0.926 | epoch 8 | 500/ 898 batches | accuracy 0.930 | epoch 8 | 750/ 898 batches | accuracy 0.934 -- | end of epoch 8 | time: 27.90s | valid accuracy 0.929 -- | epoch 9 | 250/ 898 batches | accuracy 0.935 | epoch 9 | 500/ 898 batches | accuracy 0.931 | epoch 9 | 750/ 898 batches | accuracy 0.940 -- | end of epoch 9 | time: 27.90s | valid accuracy 0.934 -- | epoch 10 | 250/ 898 batches | accuracy 0.937 | epoch 10 | 500/ 898 batches | accuracy 0.938 | epoch 10 | 750/ 898 batches | accuracy 0.942 -- | end of epoch 10 | time: 27.90s | valid accuracy 0.939 -- . print(&#39;Checking the results of test dataset.&#39;) accu_test = evaluate(test_dataloader, confu_matri=True) print(&#39;test accuracy {:8.3f}&#39;.format(accu_test)) . Checking the results of test dataset. . test accuracy 0.943 . news_label = {1: &quot;Fake&quot;, 0: &quot;True&quot;} def predict(text, text_pipeline): with torch.no_grad(): text = torch.tensor(text_pipeline(text)) output = model(text, torch.tensor([0])) return (output &gt; 0).int().item() ex_text_str = &quot;Boseman died on Aug. 28, 2020, after a years-long battle with cancer. According to representatives for the actor, he was initially diagnosed in 2016. His passing at the age of 43 sparked a conversation on building awareness about colon cancer, the disease he suffered from.&quot; model = model.to(&quot;cpu&quot;) print(&quot;This is a %s news&quot; %news_label[predict(ex_text_str, text_pipeline)]) . This is a True news . Predict using LSTM Model . # !unzip -q glove.6B.zip . import torchtext from torchtext.vocab import build_vocab_from_iterator from torchtext.vocab import Vocab import gensim.downloader from torch.utils.data import DataLoader . # embeddings_index = {} # with open(path_to_glove_file) as f: # for line in f: # word, coefs = line.split(maxsplit=1) # coefs = np.fromstring(coefs, &quot;f&quot;, sep=&quot; &quot;) # embeddings_index[word] = coefs # print(&quot;Found %s word vectors.&quot; % len(embeddings_index)) . Pre-trained Word Embedding . glove_vec = torchtext.vocab.GloVe(name=&#39;6B&#39;, dim=100) . .vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s] 100%|█████████▉| 399999/400000 [00:21&lt;00:00, 18284.74it/s] . !pip install datasets . Collecting datasets Downloading datasets-1.17.0-py3-none-any.whl (306 kB) |████████████████████████████████| 306 kB 5.4 MB/s Requirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3) Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0) Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2) Requirement already satisfied: pyarrow!=4.0.0,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0) Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4) Collecting fsspec[http]&gt;=2021.05.0 Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 10.0 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3) Collecting aiohttp Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB) |████████████████████████████████| 1.1 MB 10.1 MB/s Collecting xxhash Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB) |████████████████████████████████| 243 kB 54.0 MB/s Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5) Collecting huggingface-hub&lt;1.0.0,&gt;=0.1.0 Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB) |████████████████████████████████| 61 kB 560 kB/s Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets) (3.13) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets) (3.10.0.2) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets) (3.4.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;datasets) (3.0.6) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2021.10.8) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.10) Collecting frozenlist&gt;=1.1.1 Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB) |████████████████████████████████| 192 kB 52.2 MB/s Collecting multidict&lt;7.0,&gt;=4.5 Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB) |████████████████████████████████| 160 kB 53.8 MB/s Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (21.2.0) Collecting async-timeout&lt;5.0,&gt;=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Collecting aiosignal&gt;=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (2.0.10) Collecting asynctest==0.13.0 Downloading asynctest-0.13.0-py3-none-any.whl (26 kB) Collecting yarl&lt;2.0,&gt;=1.0 Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB) |████████████████████████████████| 271 kB 51.4 MB/s Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;datasets) (3.6.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets) (2.8.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;datasets) (1.15.0) Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2 . Prepare Dataset . import pyarrow as pa import pyarrow.dataset as ds import pandas as pd from datasets import Dataset from torch import nn import torch hg_dataset = Dataset(pa.Table.from_pandas(df[[&#39;authenticity&#39;, &#39;clean_text&#39;]])) . hg_dataset . Dataset({ features: [&#39;authenticity&#39;, &#39;clean_text&#39;], num_rows: 44898 }) . def tokenize_data(example, tokenizer, max_length): tokens = tokenizer(example[&#39;clean_text&#39;])[:max_length] length = len(tokens) return {&#39;tokens&#39;: tokens, &#39;length&#39;: length} max_length = int(df.text_length.median()) . train_size = int(0.8 * len(hg_dataset)) test_size = len(hg_dataset) - train_size tokenizer = torchtext.data.utils.get_tokenizer(&#39;basic_english&#39;) train_data, test_data = hg_dataset.train_test_split(test_size=test_size)[&#39;train&#39;], hg_dataset.train_test_split(test_size=test_size)[&#39;test&#39;] train_data = train_data.map(tokenize_data, fn_kwargs={&#39;tokenizer&#39;: tokenizer, &#39;max_length&#39;: max_length}).filter(lambda example: example[&#39;length&#39;] != 0) test_data = test_data.map(tokenize_data, fn_kwargs={&#39;tokenizer&#39;: tokenizer, &#39;max_length&#39;: max_length}).filter(lambda example: example[&#39;length&#39;] != 0) . test_size = 0.2 train_valid_data = train_data.train_test_split(test_size=test_size) train_data = train_valid_data[&#39;train&#39;] valid_data = train_valid_data[&#39;test&#39;] . # Set max length of each news text to the median of text length of all news. special_tokens = [&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;] vocab = build_vocab_from_iterator(train_data[&#39;tokens&#39;], specials=special_tokens) unk_index = vocab[&#39;&lt;unk&gt;&#39;] pad_index = vocab[&#39;&lt;pad&gt;&#39;] #make default index name the same as index of &lt;unk&gt; token vocab.set_default_index(unk_index) def numericalize_data(example, vocab): ids = [vocab[token] for token in example[&#39;tokens&#39;]] return {&#39;ids&#39;: ids} train_data = train_data.map(numericalize_data, fn_kwargs={&#39;vocab&#39;: vocab}) valid_data = valid_data.map(numericalize_data, fn_kwargs={&#39;vocab&#39;: vocab}) test_data = test_data.map(numericalize_data, fn_kwargs={&#39;vocab&#39;: vocab}) . train_data = train_data.with_format(type=&#39;torch&#39;, columns=[&#39;ids&#39;, &#39;authenticity&#39;, &#39;length&#39;]) valid_data = valid_data.with_format(type=&#39;torch&#39;, columns=[&#39;ids&#39;, &#39;authenticity&#39;, &#39;length&#39;]) test_data = test_data.with_format(type=&#39;torch&#39;, columns=[&#39;ids&#39;, &#39;authenticity&#39;, &#39;length&#39;]) . vocab([&#39;here&#39;, &#39;is&#39;, &#39;an&#39;, &#39;example&#39;, &#39;aaaaa&#39;]) . [153, 14, 35, 994, 0] . Model Building . class LSTMClassifier(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers ,pad_index): super(LSTMClassifier, self).__init__() self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index) self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True) self.fc = nn.Linear(hidden_dim, output_dim) # self.W = nn.Linear(n_hidden, n_class, bias=False) # self.b = nn.Parameter(torch.ones([n_class])) def forward(self, ids, length): embedded = self.embedding(ids) packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, enforce_sorted=False) # hidden_state = torch.zeros(1, len(X), n_hidden) # [num_layers(=1) * num_directions(=1), batch_size, n_hidden] # cell_state = torch.zeros(1, len(X), n_hidden) # [num_layers(=1) * num_directions(=1), batch_size, n_hidden] # outputs, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state)) outputs, (final_hidden_state, final_cell_state) = self.lstm(packed_embedded) # outputs = outputs[-1] # [batch_size, n_hidden] # model = self.W(outputs) + self.b # model : [batch_size, n_class] prediction = self.fc(final_hidden_state[-1]) return prediction . vocab_size = len(vocab) embedding_dim = 100 hidden_dim = 300 # output_dim = len(train_data.unique(&#39;authenticity&#39;)) output_dim = 1 n_layers = 2 model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, pad_index) . def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) print(f&#39;The model has {count_parameters(model):,} trainable parameters&#39;) . The model has 14,347,701 trainable parameters . def initialize_weights(m): if isinstance(m, nn.Linear): nn.init.xavier_normal_(m.weight) nn.init.zeros_(m.bias) elif isinstance(m, nn.LSTM): for name, param in m.named_parameters(): if &#39;bias&#39; in name: nn.init.zeros_(param) elif &#39;weight&#39; in name: nn.init.orthogonal_(param) . model.apply(initialize_weights) . LSTMClassifier( (embedding): Embedding(131426, 100, padding_idx=1) (lstm): LSTM(100, 300, num_layers=2, batch_first=True) (fc): Linear(in_features=300, out_features=1, bias=True) ) . pretrained_embedding = glove_vec.get_vecs_by_tokens(vocab.get_itos()) model.embedding.weight.data = pretrained_embedding model.embedding.weight.requires_grad = False . def collate(batch, pad_index): batch_ids = [i[&#39;ids&#39;] for i in batch] batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True) batch_length = [i[&#39;length&#39;] for i in batch] batch_length = torch.stack(batch_length) batch_label = [i[&#39;authenticity&#39;] for i in batch] batch_label = torch.stack(batch_label) batch = {&#39;ids&#39;: batch_ids, &#39;length&#39;: batch_length, &#39;authenticity&#39;: batch_label} return batch . def train(dataloader, model, criterion, optimizer, device): model.train() epoch_losses = [] epoch_accs = [] for batch in tqdm.tqdm(dataloader, desc=&#39;training...&#39;, file=sys.stdout): ids = batch[&#39;ids&#39;].to(device) length = batch[&#39;length&#39;] label = batch[&#39;authenticity&#39;].to(device) prediction = model(ids, length) loss = criterion(prediction, label.unsqueeze(1)) accuracy = get_accuracy(prediction, label) optimizer.zero_grad() loss.backward() optimizer.step() epoch_losses.append(loss.item()) epoch_accs.append(accuracy.item()) return epoch_losses, epoch_accs def evaluate(dataloader, model, criterion, device, confu_matri=False): model.eval() epoch_losses = [] epoch_accs = [] y_pred = [] y_test = [] with torch.no_grad(): for batch in tqdm.tqdm(dataloader, desc=&#39;evaluating...&#39;, file=sys.stdout): ids = batch[&#39;ids&#39;].to(device) length = batch[&#39;length&#39;] label = batch[&#39;authenticity&#39;].to(device) prediction = model(ids, length) loss = criterion(prediction, label.unsqueeze(1)) accuracy = get_accuracy(prediction, label) epoch_losses.append(loss.item()) epoch_accs.append(accuracy.item()) prediction = (prediction &gt;= 0).float() y_pred.append(prediction) y_test.append(label) if confu_matri == True: y_pred = torch.cat(y_pred, dim=0) y_test = torch.cat(y_test, dim=0) sns.heatmap(confusion_matrix(y_test.cpu().numpy(), y_pred.cpu().numpy()), annot=True) plt.show() return epoch_losses, epoch_accs . import functools from torchtext.data.functional import to_map_style_dataset device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) #Hyperparameters EPOCH = 10 LR = 0.05 # BATCH_SIZE = 64 BATCH_SIZE = 32 collate = functools.partial(collate, pad_index=pad_index) # criterion = torch.nn.CrossEntropyLoss() criterion = torch.nn.BCEWithLogitsLoss() criterion = criterion.to(device) model = model.to(device) optimizer = torch.optim.SGD(model.parameters(), lr=LR) total_accu = None #make 0.95 of trainset to be train data and 0.05 of trainset to be valid_data train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate) valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate) test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate) . def get_accuracy(prediction, label): batch_size, _ = prediction.shape predicted_class = (prediction &gt;= 0).float() correct_predictions = torch.squeeze(predicted_class).eq(label).sum() accuracy = correct_predictions / batch_size return accuracy . Training . import tqdm import sys n_epochs = 10 best_valid_loss = float(&#39;inf&#39;) train_losses = [] train_accs = [] valid_losses = [] valid_accs = [] for epoch in range(n_epochs): train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device) valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device) train_losses.extend(train_loss) train_accs.extend(train_acc) valid_losses.extend(valid_loss) valid_accs.extend(valid_acc) epoch_train_loss = np.mean(train_loss) epoch_train_acc = np.mean(train_acc) epoch_valid_loss = np.mean(valid_loss) epoch_valid_acc = np.mean(valid_acc) if epoch_valid_loss &lt; best_valid_loss: best_valid_loss = epoch_valid_loss torch.save(model.state_dict(), &#39;lstm.pt&#39;) print(f&#39;epoch: {epoch+1}&#39;) print(f&#39;train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}&#39;) print(f&#39;valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}&#39;) . training...: 100%|██████████| 884/884 [08:48&lt;00:00, 1.67it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 4.01it/s] epoch: 1 train_loss: 0.379, train_acc: 0.851 valid_loss: 0.257, valid_acc: 0.896 training...: 100%|██████████| 884/884 [08:51&lt;00:00, 1.66it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 4.00it/s] epoch: 2 train_loss: 0.210, train_acc: 0.922 valid_loss: 0.204, valid_acc: 0.934 training...: 100%|██████████| 884/884 [08:50&lt;00:00, 1.67it/s] evaluating...: 100%|██████████| 221/221 [00:54&lt;00:00, 4.05it/s] epoch: 3 train_loss: 0.180, train_acc: 0.938 valid_loss: 0.147, valid_acc: 0.949 training...: 100%|██████████| 884/884 [08:50&lt;00:00, 1.67it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 3.97it/s] epoch: 4 train_loss: 0.154, train_acc: 0.945 valid_loss: 0.101, valid_acc: 0.965 training...: 100%|██████████| 884/884 [08:51&lt;00:00, 1.66it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 3.99it/s] epoch: 5 train_loss: 0.158, train_acc: 0.945 valid_loss: 0.179, valid_acc: 0.943 training...: 100%|██████████| 884/884 [08:50&lt;00:00, 1.67it/s] evaluating...: 100%|██████████| 221/221 [00:54&lt;00:00, 4.03it/s] epoch: 6 train_loss: 0.157, train_acc: 0.944 valid_loss: 0.126, valid_acc: 0.956 training...: 100%|██████████| 884/884 [08:52&lt;00:00, 1.66it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 4.01it/s] epoch: 7 train_loss: 0.141, train_acc: 0.948 valid_loss: 0.147, valid_acc: 0.940 training...: 100%|██████████| 884/884 [08:53&lt;00:00, 1.66it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 3.99it/s] epoch: 8 train_loss: 0.166, train_acc: 0.943 valid_loss: 0.118, valid_acc: 0.951 training...: 100%|██████████| 884/884 [08:55&lt;00:00, 1.65it/s] evaluating...: 100%|██████████| 221/221 [00:54&lt;00:00, 4.04it/s] epoch: 9 train_loss: 0.157, train_acc: 0.943 valid_loss: 0.143, valid_acc: 0.949 training...: 100%|██████████| 884/884 [08:56&lt;00:00, 1.65it/s] evaluating...: 100%|██████████| 221/221 [00:55&lt;00:00, 3.96it/s] epoch: 10 train_loss: 0.150, train_acc: 0.947 valid_loss: 0.120, valid_acc: 0.957 . import matplotlib.pyplot as plt import numpy as np . fig = plt.figure(figsize=(10,6)) ax = fig.add_subplot(1,1,1) ax.plot(train_losses, label=&#39;train loss&#39;) ax.plot(valid_losses, label=&#39;valid loss&#39;) plt.legend() ax.set_xlabel(&#39;updates&#39;) ax.set_ylabel(&#39;loss&#39;); . fig = plt.figure(figsize=(10,6)) ax = fig.add_subplot(1,1,1) ax.plot(train_accs, label=&#39;train accuracy&#39;) ax.plot(valid_accs, label=&#39;valid accuracy&#39;) plt.legend() ax.set_xlabel(&#39;updates&#39;) ax.set_ylabel(&#39;accuracy&#39;); . from sklearn.metrics import confusion_matrix . model.load_state_dict(torch.load(&#39;lstm.pt&#39;)) test_loss, test_acc = evaluate(test_dataloader, model, criterion, device, confu_matri=True) epoch_test_loss = np.mean(test_loss) epoch_test_acc = np.mean(test_acc) print(f&#39;test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}&#39;) . evaluating...: 100%|██████████| 276/276 [01:10&lt;00:00, 3.92it/s] . test_loss: 0.110, test_acc: 0.960 . from pdb import set_trace . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . model_save_name = &#39;lstm.pt&#39; path = F&quot;/content/drive/My Drive/Colab Notebooks/Portfolio/{model_save_name}&quot; torch.save(model.state_dict(), path) . model_save_name = &#39;lstm.pt&#39; model.load_state_dict(torch.load(path)) . &lt;All keys matched successfully&gt; . from pdb import set_trace . def predict_sentiment(text, model, tokenizer, vocab, device, label): tokens = tokenizer(text) ids = [vocab[t] for t in tokens] length = torch.LongTensor([len(ids)]) tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device) prediction = model(tensor, length).squeeze(dim=0) probability = torch.sigmoid(prediction) predicted_class = (prediction &gt;= 0).item() predicted_probability = probability[0].item() print(&quot;This is a %s news&quot; %label[int(predicted_class)]) print(&quot;And there is a probability of %s to be a fake news&quot; %predicted_probability) . news_label = {1: &quot;Fake&quot;, 0: &quot;True&quot;} model.load_state_dict(torch.load(&#39;lstm.pt&#39;)) ex_text_str = &quot;Boseman died on Aug. 28, 2020, after a years-long battle with cancer. According to representatives for the actor, he was initially diagnosed in 2016. His passing at the age of 43 sparked a conversation on building awareness about colon cancer, the disease he suffered from.&quot; predict_sentiment(ex_text_str, model, tokenizer, vocab, device, news_label) . This is a True news And there is a probability of 0.0988902822136879 to be a fake news . ex_text_str = &quot;An autopsy revealed that the late &quot;Black Panther &quot; star Chadwick Boseman had died from poisoning.&quot; predict_sentiment(ex_text_str, model, tokenizer, vocab, device, news_label) . This is a Fake news And there is a probability of 0.5579760670661926 to be a fake news . Conclusion . Given existing set-up of embedding bag and LSTM models, LSTM model shows more accurate result. However, embeddingbag model are quicker to train and require less efforts to train such model. Therefore, depends on various business problem we might properly choose one or another model in order to maintain the system from production perspective. . Future work: . In this notebook, we found that we can achieve high accuracy in our baseline model. But we also found potential text imbalanced problem within our datasets which might accounted for its high accuracy. | Further improvement of these model could be parameter tuning, data cleaning, regularization, or more collection of unseen data. | .",
            "url": "https://liwehappy.github.io/metastability/jupyter/deep%20learning/tf-idf/text%20classification/lstm/glove/word%20embedding/pytorch/2022/01/25/Fake-News-Classification.html",
            "relUrl": "/jupyter/deep%20learning/tf-idf/text%20classification/lstm/glove/word%20embedding/pytorch/2022/01/25/Fake-News-Classification.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Text Summarization for World History Encyclopedia(a website)",
            "content": "!pip install rouge_score . Collecting rouge_score Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB) Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (0.12.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5) Requirement already satisfied: six&gt;=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5) Installing collected packages: rouge-score Successfully installed rouge-score-0.0.4 . from rouge_score import rouge_scorer from sklearn.feature_extraction.text import TfidfVectorizer from nltk import tokenize import matplotlib.pyplot as plt import html import re import random import rouge_score . Tip from Practical NLP textbook . Categories of common text summarization tasks: . Extractive vs Abstractive summarization | Query-focused vs query-independent summarization | Single-document vs query-document summarization | . Most common case: . Single document, query-independent, extractive summarizer. | . Limitation: . May need to be customized into your own use cases | ROUGE method as a metric to evaluation summarization may also need to be customized for research prorblem. | Summarization is sensitive to the size of the text given as input. A better approach would be run text summarization separately on different part of texts. | . Incentive: . Although being a well-rounded person is not an easy task, gaining more general knowledge outside of our professionals especially knowledge from world history sometimes could give us another perspective to perceive our world or have more empathy toward environments and people. By far the most effective way to be well-rounded person is by reading more books from a variety of genres. However, people nowadays can be distracted by their personal hardships and relatioships with others and therefore lack of time to read every book from beginning to end. As a result, we provide a life saver in this situation for anyone wants to learn some history from website World History Encyclopedia. In this notebook, we are going to further summarize these already abbreviated history articles from this website. This summarization will not only save your time from truely read the whole artilce but also quickly help you understand the whole articles (if you have time to read) by reading this summarization first. . Text Summarization . In this notebook, we focus on extractive methods in text summarization. . Data pre-processing . import requests from bs4 import BeautifulSoup import os.path from dateutil import parser import pandas as pd import numpy as np import re import os . BASE_DIR = &#39;/content&#39; . def download_article(url): # check if article already there filename = url.split(&#39;/&#39;)[-2] + &quot;.html&quot; os.makedirs(&#39;world_history_encyclopedia&#39;, exist_ok=True) filename = f&quot;{BASE_DIR}/world_history_encyclopedia/&quot; + filename if not os.path.isfile(filename): r = requests.get(url) with open(filename, &quot;w+&quot;) as f: f.write(r.text) return filename . def clean_article(soup_source): r = re.compile(&quot;(Sign up|news letter| n)&quot;) texts = &quot;&quot; for t in soup_source.select(&#39;p&#39;): if &#39;World History Encyclopedia&#39; in t.text: break else: if not r.match(t.text): texts += t.text + &#39; &#39; return texts . def parse_article(article_file): with open(article_file, &quot;r&quot;) as f: html = f.read() r = {} soup = BeautifulSoup(html, &#39;html.parser&#39;) r[&#39;headline&#39;] = soup.h1.text r[&#39;first_paragraph&#39;] = soup.p.text r[&#39;text&#39;] = clean_article(soup) return r . import reprlib r = reprlib.Repr() r.maxstring = 800 url1 = &quot;https://www.worldhistory.org/article/1596/mulan-the-legend-through-history/&quot; article_name1 = download_article(url1) article1 = parse_article(article_name1) print (r.repr(article1[&#39;text&#39;])) . &#34;Mulan (“magnolia”) is a legendary character in Chinese literature who is best known in the modern day from the Disney filmed adaptations (1998, 2020). Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeed... Ja Quan) which combines tai chi with dance, Kung Fu, and other arts to create a unique form of self-defense and personal improvement. The discipline is intended to encourage the confidence, strength, and grace of Mulan in modern-day practitioners and is only one of the many examples of how the legend of Mulan continues to inspire people today, especially women, just as it has done in the past. &#34; . Summarizing text using topic representation . Topic representation methods distinguish important sentences by identifying topics of sentences through important words. . Baseline: Identifying important sentences with sum of TF-IDF values . In this simple technique, we sum over the tf-idf vectors of each sentence to determine if we include this setence to be our part of our summaries. . from sklearn.feature_extraction.text import TfidfVectorizer from nltk import tokenize import nltk nltk.download(&#39;punkt&#39;) sentences = tokenize.sent_tokenize(article1[&#39;text&#39;]) tfidfVectorizer = TfidfVectorizer() words_tfidf = tfidfVectorizer.fit_transform(sentences) . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. . num_summary_sentence = 10 # Sort the sentences in descending order by the sum of TF-IDF values sent_sum = words_tfidf.sum(axis=1) important_sent = np.argsort(sent_sum, axis=0)[::-1] # Print three most important sentences in the order they appear in the article for i in range(0, len(sentences)): if i in important_sent[:num_summary_sentence]: print (sentences[i]) . Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors. The play The Female Mulan (16th century CE) modifies earlier themes, moves the action back to the time of the Northern Wei, and introduces the happy ending of the marriage motif while succeeding versions conclude with Mulan killing herself to avoid the shame of having to become the emperor&#39;s concubine until the story returned the conclusion of the joyful family reunion and marriage. The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor. In the Disney films, she is revealed as a woman but perseveres against the prohibition on females serving in the army to go beyond what is expected of her in defeating China&#39;s enemies; in the original version, where she serves 12 years, her gender is never questioned, and she only reveals herself at the end, much to the surprise of the men who served with her. The original poem takes place during the chaotic era between the fall of the Han Dynasty (202 BCE - 220 CE) and the rise of the Sui Dynasty (589-618 CE) during which China first split into the Period of the Three Kingdoms (220-280 CE) and was then ruled by succeeding short-lived dynasties, one of which was the Wei, which established itself during the period of the Northern and Southern Dynasties (386-589 CE). The author&#39;s choice of this time period and this threat is intentional to appeal to the historically literate of his audience since, in 386 CE in Northern Wei, there was unrest caused by rebels and bandits of the south and the Kingdom of Northern Wei did actually issue a call to arms and conscription. In the first act, she explains her situation, goes out and buys equipment, and then unbinds her feet (a practice which was unknown in the actual Northern Wei Period but would have been familiar to Xu Wei&#39;s audience) as the most telling detail of her transformation from female to male. Hua Hu is thus sent home, still as a man, in the company of her two fellow soldiers, to await the new appointment…As she travels with the soldiers, they comment on how strange it is that they have never seen Hua Hu use the toilet [Hua Hu responds with an allusion to the moon goddess whose face frequently changes and true self is unknown]…Returning home, Hua Hu first reapplies female makeup, then greets her family. The warrior woman Dou Xianniang also appears, combined with the “dark arts” aspect of The Story of the Loyal, Filial, and Heroic Mulan, to create an engaging character and the film works, drawing on many other aspects of the legend, to provide an audience with a new vision of the story instead of a simple remake of an earlier successful film. Among these women is Xun Guan of the Western Jin Dynasty (265-316 CE), the 13-year-old daughter of governor Xun Song, who led a hand-picked team of soldiers to break the lines of the enemy forces surrounding her father&#39;s city and brought back a relief force to lift the siege. . from sklearn.feature_extraction.text import TfidfVectorizer from nltk import tokenize def tfidf_summary(text, num_summary_sentence): summary_sentence = [] sentences = tokenize.sent_tokenize(text) tfidfVectorizer = TfidfVectorizer() words_tfidf = tfidfVectorizer.fit_transform(sentences) sentence_sum = words_tfidf.sum(axis=1) important_sentences = np.argsort(sentence_sum, axis=0)[::-1] for i in range(0, len(sentences)): if i in important_sentences[:num_summary_sentence]: summary_sentence.append(sentences[i]) return summary_sentence . print(&quot;Tf-IDF method:&quot;) tfidf_summary(article1[&#39;text&#39;], 10) . Tf-IDF method: . [&#34;Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors.&#34;, &#34;The play The Female Mulan (16th century CE) modifies earlier themes, moves the action back to the time of the Northern Wei, and introduces the happy ending of the marriage motif while succeeding versions conclude with Mulan killing herself to avoid the shame of having to become the emperor&#39;s concubine until the story returned the conclusion of the joyful family reunion and marriage.&#34;, &#34;The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor.&#34;, &#34;In the Disney films, she is revealed as a woman but perseveres against the prohibition on females serving in the army to go beyond what is expected of her in defeating China&#39;s enemies; in the original version, where she serves 12 years, her gender is never questioned, and she only reveals herself at the end, much to the surprise of the men who served with her.&#34;, &#39;The original poem takes place during the chaotic era between the fall of the Han Dynasty (202 BCE - 220 CE) and the rise of the Sui Dynasty (589-618 CE) during which China first split into the Period of the Three Kingdoms (220-280 CE) and was then ruled by succeeding short-lived dynasties, one of which was the Wei, which established itself during the period of the Northern and Southern Dynasties (386-589 CE).&#39;, &#34;The author&#39;s choice of this time period and this threat is intentional to appeal to the historically literate of his audience since, in 386 CE in Northern Wei, there was unrest caused by rebels and bandits of the south and the Kingdom of Northern Wei did actually issue a call to arms and conscription.&#34;, &#34;In the first act, she explains her situation, goes out and buys equipment, and then unbinds her feet (a practice which was unknown in the actual Northern Wei Period but would have been familiar to Xu Wei&#39;s audience) as the most telling detail of her transformation from female to male.&#34;, &#39;Hua Hu is thus sent home, still as a man, in the company of her two fellow soldiers, to await the new appointment…As she travels with the soldiers, they comment on how strange it is that they have never seen Hua Hu use the toilet [Hua Hu responds with an allusion to the moon goddess whose face frequently changes and true self is unknown]…Returning home, Hua Hu first reapplies female makeup, then greets her family.&#39;, &#39;The warrior woman Dou Xianniang also appears, combined with the “dark arts” aspect of The Story of the Loyal, Filial, and Heroic Mulan, to create an engaging character and the film works, drawing on many other aspects of the legend, to provide an audience with a new vision of the story instead of a simple remake of an earlier successful film.&#39;, &#34;Among these women is Xun Guan of the Western Jin Dynasty (265-316 CE), the 13-year-old daughter of governor Xun Song, who led a hand-picked team of soldiers to break the lines of the enemy forces surrounding her father&#39;s city and brought back a relief force to lift the siege.&#34;] . LSA algorithm . LSA essentiallu perform SVD technique from linear algebra to simply the original term frequency sentence matrix into matrices with essence of the article. . !pip install sumy . Collecting sumy Downloading sumy-0.9.0-py2.py3-none-any.whl (87 kB) |████████████████████████████████| 87 kB 3.1 MB/s Requirement already satisfied: nltk&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sumy) (3.2.5) Collecting pycountry&gt;=18.2.23 Downloading pycountry-22.1.10.tar.gz (10.1 MB) |████████████████████████████████| 10.1 MB 12.0 MB/s Requirement already satisfied: requests&gt;=2.7.0 in /usr/local/lib/python3.7/dist-packages (from sumy) (2.23.0) Collecting breadability&gt;=0.1.20 Downloading breadability-0.1.20.tar.gz (32 kB) Requirement already satisfied: docopt&lt;0.7,&gt;=0.6.1 in /usr/local/lib/python3.7/dist-packages (from sumy) (0.6.2) Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from breadability&gt;=0.1.20-&gt;sumy) (3.0.4) Requirement already satisfied: lxml&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from breadability&gt;=0.1.20-&gt;sumy) (4.2.6) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk&gt;=3.0.2-&gt;sumy) (1.15.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pycountry&gt;=18.2.23-&gt;sumy) (57.4.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.7.0-&gt;sumy) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.7.0-&gt;sumy) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.7.0-&gt;sumy) (2021.10.8) Building wheels for collected packages: breadability, pycountry Building wheel for breadability (setup.py) ... done Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21712 sha256=8c2efe92297f4a16b1794c0ff890d3c3f7c78eb61d303e8da90d0b3908e3bebc Stored in directory: /root/.cache/pip/wheels/d4/bf/51/81d27ad638e1a6dca4f362ecc33d1e2c764b8ea7ec751b8fc1 Building wheel for pycountry (setup.py) ... done Created wheel for pycountry: filename=pycountry-22.1.10-py2.py3-none-any.whl size=10595784 sha256=515490ffd056d86e68b7a5ff280929df98706deb708b59d51a2df07a3e43486f Stored in directory: /root/.cache/pip/wheels/f7/8f/9c/b070d7376caf2beb0685bf72578106b2fd57019ed57d84f126 Successfully built breadability pycountry Installing collected packages: pycountry, breadability, sumy Successfully installed breadability-0.1.20 pycountry-22.1.10 sumy-0.9.0 . from sumy.parsers.plaintext import PlaintextParser from sumy.nlp.tokenizers import Tokenizer from sumy.nlp.stemmers import Stemmer from sumy.utils import get_stop_words from sumy.summarizers.lsa import LsaSummarizer LANGUAGE = &quot;english&quot; stemmer = Stemmer(LANGUAGE) parser = PlaintextParser.from_string(article1[&#39;text&#39;], Tokenizer(LANGUAGE)) summarizer = LsaSummarizer(stemmer) summarizer.stop_words = get_stop_words(LANGUAGE) for sentence in summarizer(parser.document, num_summary_sentence): print (str(sentence)) . Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors. Scholarly consensus is that Mulan is a fictional character, probably developed in Northern China in response to the greater independence women enjoyed there, whose legend was then revised in succeeding eras to reflect the values and challenges of the times. The original work, The Poem of Mulan, dates to the 6th century CE and reflects the influences of Mongolian-Turkic peoples on the region with a focus on filial piety the central virtue and moral of the tale. By the time the character reached the modern era, through the film Mulan Joins the Army (1939), she was a staunch nationalist, driving out foreign invaders, and her earlier virtue of filial piety had been replaced by unwavering love for her country. Since the appearance of the 1998 Disney animated film, Mulan&#39;s story has grown in popularity and drawn attention to the other women, all historically attested, who took up arms for a cause they believed in throughout China&#39;s history. The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period. The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor. According to Shiamin and Idema, an opera Mu Lan Joins the Army was written in 1903 CE which sets the action in the Han Dynasty with the Xiongnu (a tribe of warriors often associated with the Huns) serving as the antagonists. This version tells the same story but with typically Disney additions of a comical talking dragon, sentient cricket, and seemingly self-aware horse and retains the happy ending of the reunion of Mulan with her father and family and the suggestion that she will marry the handsome commander she served with. The warrior woman Wong Cong&#39;er (l. c. 1777-1798 CE) commanded the White Lotus Sect in the rebellion against the Qing Dynasty, leading an army of men and using guerilla tactics to achieve a number of significant victories. . from sumy.parsers.plaintext import PlaintextParser from sumy.nlp.tokenizers import Tokenizer from sumy.nlp.stemmers import Stemmer from sumy.utils import get_stop_words from sumy.summarizers.lsa import LsaSummarizer def lsa_summary(text, num_summary_sentence): summary_sentence = [] LANGUAGE = &quot;english&quot; stemmer = Stemmer(LANGUAGE) parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE)) summarizer = LsaSummarizer(stemmer) summarizer.stop_words = get_stop_words(LANGUAGE) for sentence in summarizer(parser.document, num_summary_sentence): summary_sentence.append(str(sentence)) return summary_sentence . print(&quot;LSA Method:&quot;) lsa_summary(article1[&#39;text&#39;], 10) . LSA Method: . [&#34;Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors.&#34;, &#39;Scholarly consensus is that Mulan is a fictional character, probably developed in Northern China in response to the greater independence women enjoyed there, whose legend was then revised in succeeding eras to reflect the values and challenges of the times.&#39;, &#39;The original work, The Poem of Mulan, dates to the 6th century CE and reflects the influences of Mongolian-Turkic peoples on the region with a focus on filial piety the central virtue and moral of the tale.&#39;, &#39;By the time the character reached the modern era, through the film Mulan Joins the Army (1939), she was a staunch nationalist, driving out foreign invaders, and her earlier virtue of filial piety had been replaced by unwavering love for her country.&#39;, &#34;Since the appearance of the 1998 Disney animated film, Mulan&#39;s story has grown in popularity and drawn attention to the other women, all historically attested, who took up arms for a cause they believed in throughout China&#39;s history.&#34;, &#39;The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period.&#39;, &#34;The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor.&#34;, &#39;According to Shiamin and Idema, an opera Mu Lan Joins the Army was written in 1903 CE which sets the action in the Han Dynasty with the Xiongnu (a tribe of warriors often associated with the Huns) serving as the antagonists.&#39;, &#39;This version tells the same story but with typically Disney additions of a comical talking dragon, sentient cricket, and seemingly self-aware horse and retains the happy ending of the reunion of Mulan with her father and family and the suggestion that she will marry the handsome commander she served with.&#39;, &#34;The warrior woman Wong Cong&#39;er (l. c. 1777-1798 CE) commanded the White Lotus Sect in the rebellion against the Qing Dynasty, leading an army of men and using guerilla tactics to achieve a number of significant victories.&#34;] . Summarizing text using an indicator representation . Basically, indicator representation technique create intermediate featuress between sentences to take into account their relationship instead of using only words in each sentence. . TextRank algorithm . TextRank technique is inspired by Google&#39;s graph-based ranking algorithm. In the case of natural language text, the author of this technique build a graph associated with the text. Original paper can be found here . from sumy.summarizers.text_rank import TextRankSummarizer parser = PlaintextParser.from_string(article1[&#39;text&#39;], Tokenizer(LANGUAGE)) summarizer = TextRankSummarizer(stemmer) summarizer.stop_words = get_stop_words(LANGUAGE) for sentence in summarizer(parser.document, num_summary_sentence): print (str(sentence)) . The play The Female Mulan (16th century CE) modifies earlier themes, moves the action back to the time of the Northern Wei, and introduces the happy ending of the marriage motif while succeeding versions conclude with Mulan killing herself to avoid the shame of having to become the emperor&#39;s concubine until the story returned the conclusion of the joyful family reunion and marriage. The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period. The original poem takes place during the chaotic era between the fall of the Han Dynasty (202 BCE - 220 CE) and the rise of the Sui Dynasty (589-618 CE) during which China first split into the Period of the Three Kingdoms (220-280 CE) and was then ruled by succeeding short-lived dynasties, one of which was the Wei, which established itself during the period of the Northern and Southern Dynasties (386-589 CE). There are no known versions of the Mulan story between the Tang Dynasty and the play The Female Mulan by the playwright Xu Wei (l. 1521-1593 CE) so it is assumed that Xu Wei drew on Guo Maoqian&#39;s work for his own. In this version, Mulan is the daughter of a Turkish father and Chinese mother and, when the Turkish khan conscripts her father, Mulan goes in his place. This same story is told, with significant differences, in The Story of the Loyal, Filial, and Heroic Mulan (also known as The Complete Account of Extraordinary Mulan, c. 1800 CE). The Mulan legend was first featured on film in China in 1939 in Mulan Joins the Army which presented her as an ardent nationalist whose zeal for her country and heroism shames her male comrades throughout her tour of duty. Mulan was featured, or alluded to, in other Chinese films afterwards but received international attention in 1998 with Disney&#39;s release of the animated feature film Mulan. The warrior woman Dou Xianniang also appears, combined with the “dark arts” aspect of The Story of the Loyal, Filial, and Heroic Mulan, to create an engaging character and the film works, drawing on many other aspects of the legend, to provide an audience with a new vision of the story instead of a simple remake of an earlier successful film. Mulan&#39;s story, and name, were also influential in the establishment of the martial art of Mulan Quan (also Mulan Hua Ja Quan) which combines tai chi with dance, Kung Fu, and other arts to create a unique form of self-defense and personal improvement. . from sumy.parsers.plaintext import PlaintextParser from sumy.nlp.tokenizers import Tokenizer from sumy.nlp.stemmers import Stemmer from sumy.utils import get_stop_words from sumy.summarizers.text_rank import TextRankSummarizer def textrank_summary(text, num_summary_sentence): summary_sentence = [] LANGUAGE = &quot;english&quot; stemmer = Stemmer(LANGUAGE) parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE)) summarizer = TextRankSummarizer(stemmer) summarizer.stop_words = get_stop_words(LANGUAGE) for sentence in summarizer(parser.document, num_summary_sentence): summary_sentence.append(str(sentence)) return summary_sentence . textrank_summary(article1[&#39;text&#39;], 10) . [&#34;The play The Female Mulan (16th century CE) modifies earlier themes, moves the action back to the time of the Northern Wei, and introduces the happy ending of the marriage motif while succeeding versions conclude with Mulan killing herself to avoid the shame of having to become the emperor&#39;s concubine until the story returned the conclusion of the joyful family reunion and marriage.&#34;, &#39;The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period.&#39;, &#39;The original poem takes place during the chaotic era between the fall of the Han Dynasty (202 BCE - 220 CE) and the rise of the Sui Dynasty (589-618 CE) during which China first split into the Period of the Three Kingdoms (220-280 CE) and was then ruled by succeeding short-lived dynasties, one of which was the Wei, which established itself during the period of the Northern and Southern Dynasties (386-589 CE).&#39;, &#34;There are no known versions of the Mulan story between the Tang Dynasty and the play The Female Mulan by the playwright Xu Wei (l. 1521-1593 CE) so it is assumed that Xu Wei drew on Guo Maoqian&#39;s work for his own.&#34;, &#39;In this version, Mulan is the daughter of a Turkish father and Chinese mother and, when the Turkish khan conscripts her father, Mulan goes in his place.&#39;, &#39;This same story is told, with significant differences, in The Story of the Loyal, Filial, and Heroic Mulan (also known as The Complete Account of Extraordinary Mulan, c. 1800 CE).&#39;, &#39;The Mulan legend was first featured on film in China in 1939 in Mulan Joins the Army which presented her as an ardent nationalist whose zeal for her country and heroism shames her male comrades throughout her tour of duty.&#39;, &#34;Mulan was featured, or alluded to, in other Chinese films afterwards but received international attention in 1998 with Disney&#39;s release of the animated feature film Mulan.&#34;, &#39;The warrior woman Dou Xianniang also appears, combined with the “dark arts” aspect of The Story of the Loyal, Filial, and Heroic Mulan, to create an engaging character and the film works, drawing on many other aspects of the legend, to provide an audience with a new vision of the story instead of a simple remake of an earlier successful film.&#39;, &#34;Mulan&#39;s story, and name, were also influential in the establishment of the martial art of Mulan Quan (also Mulan Hua Ja Quan) which combines tai chi with dance, Kung Fu, and other arts to create a unique form of self-defense and personal improvement.&#34;] . Measuring the performance of Text Summarization methods(Rouge Score) . ROUGE - Recall-Oriented Understudy for Gisting Evaluation, is a popular metrics for measuring the accuracy when dealing with langugage summarization or translation. Although ROUGE method can only measure syntactical matches rather than semantic similarities between words, it is still a very good tool at hand to handle summarization and machine translation tasks. . def print_rouge_score(rouge_score): for k,v in rouge_score.items(): print (k, &#39;Precision:&#39;, &quot;{:.2f}&quot;.format(v.precision), &#39;Recall:&#39;, &quot;{:.2f}&quot;.format(v.recall), &#39;fmeasure:&#39;, &quot;{:.2f}&quot;.format(v.fmeasure)) . Since we don&#39;t have human-generated summaries of these World History Encyclopedia articles, model evaluation below, we will use the first paragraph as the gist of each of articles from world history encyclopedia. . Example articles . class TextSummarization(): def __init__(self, url, alg, num_sentences, n_gram): assert isinstance(alg, list) == True, &quot;Candidate algorithms are not list type&quot; self.article = parse_article(download_article(url)) self.alg_list = alg self.ngram = n_gram self.num_sentences = num_summary_sentence self.standard = self.article[&#39;first_paragraph&#39;] def rouge_score(self, n_gram, raw_summary, best=False): # rouge score can also be measured based on different length of ngrams scorer = rouge_scorer.RougeScorer([f&#39;rouge{self.ngram}&#39;], use_stemmer=True) scores = scorer.score(self.standard, raw_summary) if best: _, _, fmeasure = scores[f&#39;rouge{self.ngram}&#39;] return fmeasure else: return scores def raw_summary(self): summaries = [(&#39;&#39;.join(alg(self.article[&#39;text&#39;], self.num_sentences)), alg) for alg in self.alg_list] return summaries def print_summary_detail(self): names = [] scores = [] summaries = [] for summary, alg in self.raw_summary(): names.append(alg.__name__) scores.append(self.rouge_score(self.ngram, summary).items()) summaries.append(alg(self.article[&#39;text&#39;], self.num_sentences)) # print() print(&quot;Detail:&quot;) return {n:[sc, su] for n, sc, su in zip(names, scores, summaries)} def best_summary(self): fmeasures = [self.rouge_score(self.ngram, summary, best=True) for summary, alg in self.raw_summary()] best_alg = self.alg_list[fmeasures.index(max(fmeasures))] print(*best_alg(self.article[&#39;text&#39;], self.num_sentences), sep=&#39; n&#39;) print() . Mulan: The Legend Through History . curl = &#39;https://www.worldhistory.org/article/1596/mulan-the-legend-through-history/&#39; summaries = TextSummarization(url, [tfidf_summary, lsa_summary, textrank_summary], 10, 2) summaries.best_summary() summaries.print_summary_detail() . Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors. Scholarly consensus is that Mulan is a fictional character, probably developed in Northern China in response to the greater independence women enjoyed there, whose legend was then revised in succeeding eras to reflect the values and challenges of the times. The original work, The Poem of Mulan, dates to the 6th century CE and reflects the influences of Mongolian-Turkic peoples on the region with a focus on filial piety the central virtue and moral of the tale. By the time the character reached the modern era, through the film Mulan Joins the Army (1939), she was a staunch nationalist, driving out foreign invaders, and her earlier virtue of filial piety had been replaced by unwavering love for her country. Since the appearance of the 1998 Disney animated film, Mulan&#39;s story has grown in popularity and drawn attention to the other women, all historically attested, who took up arms for a cause they believed in throughout China&#39;s history. The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period. The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor. According to Shiamin and Idema, an opera Mu Lan Joins the Army was written in 1903 CE which sets the action in the Han Dynasty with the Xiongnu (a tribe of warriors often associated with the Huns) serving as the antagonists. This version tells the same story but with typically Disney additions of a comical talking dragon, sentient cricket, and seemingly self-aware horse and retains the happy ending of the reunion of Mulan with her father and family and the suggestion that she will marry the handsome commander she served with. The warrior woman Wong Cong&#39;er (l. c. 1777-1798 CE) commanded the White Lotus Sect in the rebellion against the Qing Dynasty, leading an army of men and using guerilla tactics to achieve a number of significant victories. Detail: . {&#39;lsa_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.07092198581560284, recall=0.6122448979591837, fmeasure=0.12711864406779663))]), [&#34;Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors.&#34;, &#39;Scholarly consensus is that Mulan is a fictional character, probably developed in Northern China in response to the greater independence women enjoyed there, whose legend was then revised in succeeding eras to reflect the values and challenges of the times.&#39;, &#39;The original work, The Poem of Mulan, dates to the 6th century CE and reflects the influences of Mongolian-Turkic peoples on the region with a focus on filial piety the central virtue and moral of the tale.&#39;, &#39;By the time the character reached the modern era, through the film Mulan Joins the Army (1939), she was a staunch nationalist, driving out foreign invaders, and her earlier virtue of filial piety had been replaced by unwavering love for her country.&#39;, &#34;Since the appearance of the 1998 Disney animated film, Mulan&#39;s story has grown in popularity and drawn attention to the other women, all historically attested, who took up arms for a cause they believed in throughout China&#39;s history.&#34;, &#39;The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period.&#39;, &#34;The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor.&#34;, &#39;According to Shiamin and Idema, an opera Mu Lan Joins the Army was written in 1903 CE which sets the action in the Han Dynasty with the Xiongnu (a tribe of warriors often associated with the Huns) serving as the antagonists.&#39;, &#39;This version tells the same story but with typically Disney additions of a comical talking dragon, sentient cricket, and seemingly self-aware horse and retains the happy ending of the reunion of Mulan with her father and family and the suggestion that she will marry the handsome commander she served with.&#39;, &#34;The warrior woman Wong Cong&#39;er (l. c. 1777-1798 CE) commanded the White Lotus Sect in the rebellion against the Qing Dynasty, leading an army of men and using guerilla tactics to achieve a number of significant victories.&#34;]], &#39;textrank_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.015625, recall=0.14285714285714285, fmeasure=0.028169014084507043))]), [&#34;The play The Female Mulan (16th century CE) modifies earlier themes, moves the action back to the time of the Northern Wei, and introduces the happy ending of the marriage motif while succeeding versions conclude with Mulan killing herself to avoid the shame of having to become the emperor&#39;s concubine until the story returned the conclusion of the joyful family reunion and marriage.&#34;, &#39;The story, as it is best known today through the recent films, places Mulan in an unidentified era of Imperial China (221 BCE - 1912 CE), but the original poem is set during the Northern Wei Period.&#39;, &#39;The original poem takes place during the chaotic era between the fall of the Han Dynasty (202 BCE - 220 CE) and the rise of the Sui Dynasty (589-618 CE) during which China first split into the Period of the Three Kingdoms (220-280 CE) and was then ruled by succeeding short-lived dynasties, one of which was the Wei, which established itself during the period of the Northern and Southern Dynasties (386-589 CE).&#39;, &#34;There are no known versions of the Mulan story between the Tang Dynasty and the play The Female Mulan by the playwright Xu Wei (l. 1521-1593 CE) so it is assumed that Xu Wei drew on Guo Maoqian&#39;s work for his own.&#34;, &#39;In this version, Mulan is the daughter of a Turkish father and Chinese mother and, when the Turkish khan conscripts her father, Mulan goes in his place.&#39;, &#39;This same story is told, with significant differences, in The Story of the Loyal, Filial, and Heroic Mulan (also known as The Complete Account of Extraordinary Mulan, c. 1800 CE).&#39;, &#39;The Mulan legend was first featured on film in China in 1939 in Mulan Joins the Army which presented her as an ardent nationalist whose zeal for her country and heroism shames her male comrades throughout her tour of duty.&#39;, &#34;Mulan was featured, or alluded to, in other Chinese films afterwards but received international attention in 1998 with Disney&#39;s release of the animated feature film Mulan.&#34;, &#39;The warrior woman Dou Xianniang also appears, combined with the “dark arts” aspect of The Story of the Loyal, Filial, and Heroic Mulan, to create an engaging character and the film works, drawing on many other aspects of the legend, to provide an audience with a new vision of the story instead of a simple remake of an earlier successful film.&#39;, &#34;Mulan&#39;s story, and name, were also influential in the establishment of the martial art of Mulan Quan (also Mulan Hua Ja Quan) which combines tai chi with dance, Kung Fu, and other arts to create a unique form of self-defense and personal improvement.&#34;]], &#39;tfidf_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.04833333333333333, recall=0.5918367346938775, fmeasure=0.08936825885978429))]), [&#34;Her story, however, about a young girl who takes her father&#39;s place in the army to help save her country, is hundreds of years old The tale most likely originated in the Northern Wei Period (386-535 CE) of China before it was developed by succeeding authors.&#34;, &#34;The play The Female Mulan (16th century CE) modifies earlier themes, moves the action back to the time of the Northern Wei, and introduces the happy ending of the marriage motif while succeeding versions conclude with Mulan killing herself to avoid the shame of having to become the emperor&#39;s concubine until the story returned the conclusion of the joyful family reunion and marriage.&#34;, &#34;The modern and ancient versions follow basically the same plot of a young girl who takes her aged father&#39;s place in the army when he is called to serve, performs her duties admirably, saves her country, and returns home to her family where she is received with honor.&#34;, &#34;In the Disney films, she is revealed as a woman but perseveres against the prohibition on females serving in the army to go beyond what is expected of her in defeating China&#39;s enemies; in the original version, where she serves 12 years, her gender is never questioned, and she only reveals herself at the end, much to the surprise of the men who served with her.&#34;, &#39;The original poem takes place during the chaotic era between the fall of the Han Dynasty (202 BCE - 220 CE) and the rise of the Sui Dynasty (589-618 CE) during which China first split into the Period of the Three Kingdoms (220-280 CE) and was then ruled by succeeding short-lived dynasties, one of which was the Wei, which established itself during the period of the Northern and Southern Dynasties (386-589 CE).&#39;, &#34;The author&#39;s choice of this time period and this threat is intentional to appeal to the historically literate of his audience since, in 386 CE in Northern Wei, there was unrest caused by rebels and bandits of the south and the Kingdom of Northern Wei did actually issue a call to arms and conscription.&#34;, &#34;In the first act, she explains her situation, goes out and buys equipment, and then unbinds her feet (a practice which was unknown in the actual Northern Wei Period but would have been familiar to Xu Wei&#39;s audience) as the most telling detail of her transformation from female to male.&#34;, &#39;Hua Hu is thus sent home, still as a man, in the company of her two fellow soldiers, to await the new appointment…As she travels with the soldiers, they comment on how strange it is that they have never seen Hua Hu use the toilet [Hua Hu responds with an allusion to the moon goddess whose face frequently changes and true self is unknown]…Returning home, Hua Hu first reapplies female makeup, then greets her family.&#39;, &#39;The warrior woman Dou Xianniang also appears, combined with the “dark arts” aspect of The Story of the Loyal, Filial, and Heroic Mulan, to create an engaging character and the film works, drawing on many other aspects of the legend, to provide an audience with a new vision of the story instead of a simple remake of an earlier successful film.&#39;, &#34;Among these women is Xun Guan of the Western Jin Dynasty (265-316 CE), the 13-year-old daughter of governor Xun Song, who led a hand-picked team of soldiers to break the lines of the enemy forces surrounding her father&#39;s city and brought back a relief force to lift the siege.&#34;]]} . Genghis Khan . url = &#39;https://www.worldhistory.org/Genghis_Khan/&#39; summaries = TextSummarization(url, [tfidf_summary, lsa_summary, textrank_summary], 10, 2) summaries.best_summary() summaries.print_summary_detail() . Genghis Khan (aka Chinggis Khan) was the founder of the Mongol Empire which he ruled from 1206 until his death in 1227. Born Temujin, he acquired the title of Genghis Khan, likely meaning &#39;universal ruler’, after unifying the Mongol tribes. Genghis Khan had a fearsome reputation but he was an able administrator who introduced writing to the Mongols, created their first law code, promoted trade and granted religious freedom by permitting all religions to be freely practised anywhere in the Mongol world. Temujin&#39;s mother was called Hoelun and his father, Yisugei, who was a tribal leader, and he arranged for his son to marry Borte (aka Bortei), the daughter of another influential Mongol leader, Dei-secen, but before this plan could come to fruition, Temujin&#39;s father was poisoned by a rival. As his army swelled to ever greater proportions, Temujin defeated, over a period of ten years or so, such rivals as the Tartars, Kereyids, Naimans, and Merkids until a Mongol confederation met at a great conference or kurultai at the Kerulen river in 1206 and formally declared Temujin their leader. He was given the title of Genghis Khan which likely means &#39;universal&#39; leader (the spelling in Mongol is Chinggis but &#39;Genghis&#39; remains more familiar today and derives from medieval Arab scholars not having a ch in their language). The Jin State Genghis attacked the Jin state (aka Jurchen Jin Dynasty, 1115-1234) and the plain of the Yellow River in 1205, 1209, and 1211, the latter invasion consisting of two Mongol armies of 50,000 men each. Rather short-sightedly, the fourth player in this game of empires, the Chinese Song Dynasty (aka Sung, 960 - 1279), instead of allying with the Jin to create a useful buffer zone between themselves and the Mongols, allied with the Khan. 1193), who made the following description of the Great Khan, by then already a legendary figure: A man of a tall stature, of vigorous build, robust in body, the hair on his face scanty and turned white, with cat&#39;s eyes, possessed of great energy, discernment, genius and understanding, awe-inspiring, a butcher, just, resolute, an overthrower of enemies, intrepid, sanguinary and cruel&#39; (Tabakat-i Nasiri, c. 1260, quoted in Saunders, 63) Genghis Khan died on 18 August 1227 of an unknown illness, perhaps initially caused by falling from his horse while hunting a few months earlier. At the time, he was back in northwest China besieging the Xia state&#39;s capital, Zhongxing, and the news of the great leader&#39;s death was kept from the Mongol army until the city had capitulated and its inhabitants been slaughtered. Detail: . {&#39;lsa_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.02522935779816514, recall=0.1896551724137931, fmeasure=0.04453441295546559))]), [&#39;Temujin proved unstoppable, though, and he managed to unify most of the different nomadic tribes which roamed the grasslands of central Asia, each one composed of different but related clans, by creating a web of alliances between them.&#39;, &#34;He was given the title of Genghis Khan which likely means &#39;universal&#39; leader (the spelling in Mongol is Chinggis but &#39;Genghis&#39; remains more familiar today and derives from medieval Arab scholars not having a ch in their language).&#34;, &#39;Another innovation was the development of a postal system where horse-riding couriers could quickly carry messages across long distances and who were provided with regular stations for food, rest, and a change of horse.&#39;, &#39;One Jin official, Yuan Haowen (1190-1257) wrote the following poem to describe the devastation of the Mongol invasion: White bones scattered like tangled hemp, how soon before mulberry and catalpa turn to dragon-sands?&#39;, &#34;(Ebrey, 237) To add to the Jin&#39;s woes, they were beset with internal problems such as chronic corruption emptying the state coffers, natural disasters, and assassinations of top officials, including Emperor Feidi in 1213.&#34;, &#39;Song China was now fully exposed from the north and weaker than ever, wracked as it was by internal political factions and hamstrung by an overly conservative foreign policy which meant that it was only a matter of time before the Mongols brought about its collapse, too.&#39;, &#39;The campaigns in Western Asia and the edge of Europe brought Genghis Khan and the Mongols to the attention of a different set of historians than the Chinese, notably the Persian Minhaj al-Siraj Juzjani (b.&#39;, &#34;1193), who made the following description of the Great Khan, by then already a legendary figure: A man of a tall stature, of vigorous build, robust in body, the hair on his face scanty and turned white, with cat&#39;s eyes, possessed of great energy, discernment, genius and understanding, awe-inspiring, a butcher, just, resolute, an overthrower of enemies, intrepid, sanguinary and cruel&#39; (Tabakat-i Nasiri, c. 1260, quoted in Saunders, 63) Genghis Khan died on 18 August 1227 of an unknown illness, perhaps initially caused by falling from his horse while hunting a few months earlier.&#34;, &#34;At the time, he was back in northwest China besieging the Xia state&#39;s capital, Zhongxing, and the news of the great leader&#39;s death was kept from the Mongol army until the city had capitulated and its inhabitants been slaughtered.&#34;, &#39;Medieval sources mention the tomb was in the vicinity of the sacred mountain Burkan Kuldun, and that his son Ogedei sacrificed 40 slave girls and 40 horses to accompany his father into the next life.&#39;]], &#39;textrank_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.08924485125858124, recall=0.6724137931034483, fmeasure=0.15757575757575756))]), [&#39;Genghis Khan (aka Chinggis Khan) was the founder of the Mongol Empire which he ruled from 1206 until his death in 1227.&#39;, &#34;Born Temujin, he acquired the title of Genghis Khan, likely meaning &#39;universal ruler’, after unifying the Mongol tribes.&#34;, &#39;Genghis Khan had a fearsome reputation but he was an able administrator who introduced writing to the Mongols, created their first law code, promoted trade and granted religious freedom by permitting all religions to be freely practised anywhere in the Mongol world.&#39;, &#34;Temujin&#39;s mother was called Hoelun and his father, Yisugei, who was a tribal leader, and he arranged for his son to marry Borte (aka Bortei), the daughter of another influential Mongol leader, Dei-secen, but before this plan could come to fruition, Temujin&#39;s father was poisoned by a rival.&#34;, &#39;As his army swelled to ever greater proportions, Temujin defeated, over a period of ten years or so, such rivals as the Tartars, Kereyids, Naimans, and Merkids until a Mongol confederation met at a great conference or kurultai at the Kerulen river in 1206 and formally declared Temujin their leader.&#39;, &#34;He was given the title of Genghis Khan which likely means &#39;universal&#39; leader (the spelling in Mongol is Chinggis but &#39;Genghis&#39; remains more familiar today and derives from medieval Arab scholars not having a ch in their language).&#34;, &#39;The Jin State Genghis attacked the Jin state (aka Jurchen Jin Dynasty, 1115-1234) and the plain of the Yellow River in 1205, 1209, and 1211, the latter invasion consisting of two Mongol armies of 50,000 men each.&#39;, &#39;Rather short-sightedly, the fourth player in this game of empires, the Chinese Song Dynasty (aka Sung, 960 - 1279), instead of allying with the Jin to create a useful buffer zone between themselves and the Mongols, allied with the Khan.&#39;, &#34;1193), who made the following description of the Great Khan, by then already a legendary figure: A man of a tall stature, of vigorous build, robust in body, the hair on his face scanty and turned white, with cat&#39;s eyes, possessed of great energy, discernment, genius and understanding, awe-inspiring, a butcher, just, resolute, an overthrower of enemies, intrepid, sanguinary and cruel&#39; (Tabakat-i Nasiri, c. 1260, quoted in Saunders, 63) Genghis Khan died on 18 August 1227 of an unknown illness, perhaps initially caused by falling from his horse while hunting a few months earlier.&#34;, &#34;At the time, he was back in northwest China besieging the Xia state&#39;s capital, Zhongxing, and the news of the great leader&#39;s death was kept from the Mongol army until the city had capitulated and its inhabitants been slaughtered.&#34;]], &#39;tfidf_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.019305019305019305, recall=0.1724137931034483, fmeasure=0.034722222222222224))]), [&#39;Genghis Khan had a fearsome reputation but he was an able administrator who introduced writing to the Mongols, created their first law code, promoted trade and granted religious freedom by permitting all religions to be freely practised anywhere in the Mongol world.&#39;, &#34;Temujin&#39;s mother was called Hoelun and his father, Yisugei, who was a tribal leader, and he arranged for his son to marry Borte (aka Bortei), the daughter of another influential Mongol leader, Dei-secen, but before this plan could come to fruition, Temujin&#39;s father was poisoned by a rival.&#34;, &#39;Things then got even worse when the young Temujin was captured by a rival clan leader, perhaps following an incident where Temujin may have killed one of his older half-brothers, Bekter, who likely represented a rival branch of the family that had taken on the legacy of Yisugei.&#39;, &#34;Courageous himself in battle, Temujin would often reward bravery shown by the defeated, famously making a man called Jebe one of his generals because he had withstood a cavalry charge and fired an arrow that felled Temujin&#39;s own horse.&#34;, &#39;As his army swelled to ever greater proportions, Temujin defeated, over a period of ten years or so, such rivals as the Tartars, Kereyids, Naimans, and Merkids until a Mongol confederation met at a great conference or kurultai at the Kerulen river in 1206 and formally declared Temujin their leader.&#39;, &#39;The aim now was to combine this power base with the traditional Mongol skills of horsemanship and archery, and, not only overcome tradition rival neighbouring states but build an empire which then could conquer the richest state in Asia: China.&#39;, &#39;They not only brought ferocious mobility to Asian warfare but they were, thanks to their flexibility, quickly adept at other types of battle, too, like siege warfare and the use of gunpowder missiles and catapults (first Chinese ones and then those from Afghanistan were adopted when they realised they were superior).&#39;, &#39;Song China was now fully exposed from the north and weaker than ever, wracked as it was by internal political factions and hamstrung by an overly conservative foreign policy which meant that it was only a matter of time before the Mongols brought about its collapse, too.&#39;, &#34;1193), who made the following description of the Great Khan, by then already a legendary figure: A man of a tall stature, of vigorous build, robust in body, the hair on his face scanty and turned white, with cat&#39;s eyes, possessed of great energy, discernment, genius and understanding, awe-inspiring, a butcher, just, resolute, an overthrower of enemies, intrepid, sanguinary and cruel&#39; (Tabakat-i Nasiri, c. 1260, quoted in Saunders, 63) Genghis Khan died on 18 August 1227 of an unknown illness, perhaps initially caused by falling from his horse while hunting a few months earlier.&#34;, &#39;The empire was to be divided amongst his sons Jochi, Chagatai (Chaghadai), Tolui (Tului), and Ogedei (Ogodei), with each ruling a khanate (although Jochi would predecease his father in 1227) and Ogedei, the third son, becoming the new Great Khan in 1229, a position he would maintain until his death in 1241.&#39;]]} . The Iberian Conquest of the Americas . url = &#39;https://www.worldhistory.org/article/1920/the-iberian-conquest-of-the-americas/&#39; summaries = TextSummarization(url, [tfidf_summary, lsa_summary, textrank_summary], 10, 2) summaries.best_summary() summaries.print_summary_detail() . European explorers began to probe the Western Hemisphere in the early 1500s, and they found to their utter amazement not only a huge landmass but also a world filled with several diverse and populous indigenous cultures. Among their most important conquests were those of Christopher Columbus in the Caribbean (1492-1502); Hernán Cortés in Aztec Mexico (1519-1521), Francisco Pizzaro and Diego de Almagro in Inca Peru (1528-1532), and Juan de Grijalva (1518) and Hernán Cortés (1519; 1524-1525) in Mayan Yucatán and Guatemala. Following an earlier expedition through the Yucatán led by Juan de Grijalva, Hernán Cortés began his campaign against the Aztec Empire in 1519 and with his coalition army captured the emperor Cuauhtémoc and the capital of the Aztec Empire, Tenochtitlan in 1521. It began when Francisco Pizarro, with his Andean allies captured and strangled Emperor Atahualpa in 1532, but it did not end for another 40 years until the last Inca stronghold of Vilcabamba (1500 m northeast of Cuzco) was conquered in 1572. Following Cortés&#39; conquest of the Aztec Empire, the Crown of Castile established the Kingdom of New Spain, which covered a huge area including what is now Mexico, much of the American Southwest in North America, Central America, northern parts of South America, and the Philippines. The Spanish enforced this system of stratification through three institutions: When soldiers first encountered the native peoples, they were supposed to read aloud in Spanish the requerimiento, which told the natives they had to submit to the authority of the Spanish crown or face the sword. Some have argued that haciendas evolved from the encomiendas, but in the haciendas system, land was granted to private individuals, but instead of the owners utilizing forced labor on these estates, free labor was recruited on a permanent or casual basis. Because of the failure of most of the captaincies and the continued menace of French ships along the coast, King John III of Portugal (r. 1521-1557) decided in 1549 that he would have to colonize Brazil as a royal enterprise. The Amerindians were confronted with an array of crops of which they had no experience and were left with insufficient time to grow their own crops while tending the fields of their masters or working in dank, dark mines. There were at least 60 million people in North, Central, and South America before the first European contact in 1492, and by the 1600s probably 56 million had died, 90% of the whole pre-Columbian indigenous population. Detail: . {&#39;lsa_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.10434782608695652, recall=0.4044943820224719, fmeasure=0.16589861751152074))]), [&#39;European explorers began to probe the Western Hemisphere in the early 1500s, and they found to their utter amazement not only a huge landmass but also a world filled with several diverse and populous indigenous cultures.&#39;, &#39;The Amerindian farmers grew cassava, maize, potato, beans, squash, tomatoes, and chili peppers, while the Iberians cultivated wheat, barley, cabbage, onions, garlic, and carrots.&#39;, &#34;Instead, he established a settlement at La Isabela, in today&#39;s Dominican Republic, from which he explored the interior of the Island for gold and silver and imposed a brutal tribute system on the local Taínos.&#34;, &#39;It began when Francisco Pizarro, with his Andean allies captured and strangled Emperor Atahualpa in 1532, but it did not end for another 40 years until the last Inca stronghold of Vilcabamba (1500 m northeast of Cuzco) was conquered in 1572.&#39;, &#39;The Spaniards would supervise the land, run the mines, and staff the colonial administrations, and the Amerindians (la republica de los indios), would provide the labor to feed, house, and clothe the Spanish.&#39;, &#39;When some semblance of control was obtained, royal authorities then gave hereditary ownership of the native land to nobles and officers who, through the encomienda, received tribute and labor from the Indian villages.&#39;, &#39;The indigenous occupants were required to provide a tribute of anything the land proved to hold – gold, crops, foodstuffs, and animals, and they owed a portion of their time to work on plantations or in mines.&#39;, &#39;The widespread protest forced the crown to back down for a while, but in the early 1600s, the king replaced the encomienda with the repartimiento, where government officials (corregidor de indios) took over regulating the labor of the indigenous people.&#39;, &#39;Pernambuco, the most successful captaincy, belonged to Duarte Coelho, who founded the city of Olinda in 1536 and set up sugar factories and plantations, to satisfy the growing sweet tooth of Europe.&#39;, &#39;Tomé de Sousa also worked to repair the villages and reorganize the economies of the old captaincies and spread the Catholic faith among the indigenous people through the Jesuits, officially supported by the king.&#39;]], &#39;textrank_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.016172506738544475, recall=0.06741573033707865, fmeasure=0.02608695652173913))]), [&#39;Their empire was a confederation of three huge city-states established in 1427: Tenochtitlan, the capital located on an island near the western shore of Lake Texcoco in central Mexico, Texcoco in the central Mexican plateau, and Tlacopan in the Valley of Mexico on the western shore of Lake Texcoco.&#39;, &#39;Unlike the conquistadors in Spanish America who followed him, he made no large-scale conquest of the indigenous Taíno people.&#39;, &#34;Following Cortés&#39; conquest of the Aztec Empire, the Crown of Castile established the Kingdom of New Spain, which covered a huge area including what is now Mexico, much of the American Southwest in North America, Central America, northern parts of South America, and the Philippines.&#34;, &#39;The Spanish enforced this system of stratification through three institutions: When soldiers first encountered the native peoples, they were supposed to read aloud in Spanish the requerimiento, which told the natives they had to submit to the authority of the Spanish crown or face the sword.&#39;, &#39;The Spanish crown passed laws to make it clear that the indigenous people were not meant to be slaves and were in fact Spanish subjects with certain rights, but these laws were met with great hostility and resistance.&#39;, &#39;The widespread protest forced the crown to back down for a while, but in the early 1600s, the king replaced the encomienda with the repartimiento, where government officials (corregidor de indios) took over regulating the labor of the indigenous people.&#39;, &#39;The captaincies were granted control over huge strips of land and all the indigenous people that resided upon it, much like the Spanish encomiendas.&#39;, &#39;Tomé de Sousa also worked to repair the villages and reorganize the economies of the old captaincies and spread the Catholic faith among the indigenous people through the Jesuits, officially supported by the king.&#39;, &#39;During the European colonization of the Americas, the Europeans did all they could to ignore the agricultural achievements of the Amerindians and set out to replace the indigenous agro-ecosystems with their own crops and methods.&#39;, &#39;There were at least 60 million people in North, Central, and South America before the first European contact in 1492, and by the 1600s probably 56 million had died, 90% of the whole pre-Columbian indigenous population.&#39;]], &#39;tfidf_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.20892018779342722, recall=1.0, fmeasure=0.34563106796116505))]), [&#39;European explorers began to probe the Western Hemisphere in the early 1500s, and they found to their utter amazement not only a huge landmass but also a world filled with several diverse and populous indigenous cultures.&#39;, &#39;Among their most important conquests were those of Christopher Columbus in the Caribbean (1492-1502); Hernán Cortés in Aztec Mexico (1519-1521), Francisco Pizzaro and Diego de Almagro in Inca Peru (1528-1532), and Juan de Grijalva (1518) and Hernán Cortés (1519; 1524-1525) in Mayan Yucatán and Guatemala.&#39;, &#39;Following an earlier expedition through the Yucatán led by Juan de Grijalva, Hernán Cortés began his campaign against the Aztec Empire in 1519 and with his coalition army captured the emperor Cuauhtémoc and the capital of the Aztec Empire, Tenochtitlan in 1521.&#39;, &#39;It began when Francisco Pizarro, with his Andean allies captured and strangled Emperor Atahualpa in 1532, but it did not end for another 40 years until the last Inca stronghold of Vilcabamba (1500 m northeast of Cuzco) was conquered in 1572.&#39;, &#34;Following Cortés&#39; conquest of the Aztec Empire, the Crown of Castile established the Kingdom of New Spain, which covered a huge area including what is now Mexico, much of the American Southwest in North America, Central America, northern parts of South America, and the Philippines.&#34;, &#39;The Spanish enforced this system of stratification through three institutions: When soldiers first encountered the native peoples, they were supposed to read aloud in Spanish the requerimiento, which told the natives they had to submit to the authority of the Spanish crown or face the sword.&#39;, &#39;Some have argued that haciendas evolved from the encomiendas, but in the haciendas system, land was granted to private individuals, but instead of the owners utilizing forced labor on these estates, free labor was recruited on a permanent or casual basis.&#39;, &#39;Because of the failure of most of the captaincies and the continued menace of French ships along the coast, King John III of Portugal (r. 1521-1557) decided in 1549 that he would have to colonize Brazil as a royal enterprise.&#39;, &#39;The Amerindians were confronted with an array of crops of which they had no experience and were left with insufficient time to grow their own crops while tending the fields of their masters or working in dank, dark mines.&#39;, &#39;There were at least 60 million people in North, Central, and South America before the first European contact in 1492, and by the 1600s probably 56 million had died, 90% of the whole pre-Columbian indigenous population.&#39;]]} . Christmas Through the Ages . url = &#39;https://www.worldhistory.org/article/1893/christmas-through-the-ages/&#39; summaries = TextSummarization(url, [tfidf_summary, lsa_summary, textrank_summary], 10, 2) summaries.best_summary() summaries.print_summary_detail() . From gift-giving to the sumptuous spread of a Christmas dinner table, this article traces the history of the celebrations from Roman times to the Victorian era when our modern take on the holiday was firmly established in both deed and literature. Saturnalia was a week-long Roman festival held between the 17th and 23rd of December that honoured the agricultural god Saturn, nicely encompassing the winter solstice, another event of significance in the pagan calendar. For the rich, fine clothes and jewellery were the norm; for the less well-off, nicer food than usual, a bundle of firewood or simple wooden toys like spinning tops and dolls were eagerly anticipated. The tradition of giving gifts on the 1st January remained strong, and this included Elizabeth I of England herself, who regularly received jewels, extravagant dresses, and feather fans from her courtiers. With no public roads, travelling by horse and carriage was slow and uncomfortable; nevertheless, the more intrepid could visit sights like Francis Drake&#39;s Golden Hind ship in London, which had made the first English circumnavigation of the globe (1577 to 1580). The Victorians certainly ensured that such medieval elements as a Christmas morning church service, feasting, games, gifts, and pantomimes continued to enjoy their status as essential activities of the season. The young fir tree was decorated with candles and small presents (toys, sweets, charms, and candied fruit) hung from its boughs that were destined to be distributed to the Christmas guests who might have their name tagged to their gift. A more efficient postage system and the introduction of the Penny Black stamp in 1840 meant that correspondence increased and a tradition developed of sending friends and distant family a Christmas card, first introduced in England in 1843. The jolly figure with the long white beard who visits homes on Christmas Eve to leave good children presents has his origins in the 4th-century Saint Nicholas, Bishop of Myra in Anatolia who delighted in distributing gifts, including sacks of gold. Nowadays, electric lights may have replaced the candles on the tree, churches are not quite as busy as they used to be, the Yule log is now usually chocolate, and many of the cards have become electronic, but the traditions that have run through the centuries to celebrate Christmas Day continue each and every year to enchant and inspire as they have always done. Detail: . {&#39;lsa_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.12090680100755667, recall=0.4752475247524752, fmeasure=0.19277108433734938))]), [&#39;From gift-giving to the sumptuous spread of a Christmas dinner table, this article traces the history of the celebrations from Roman times to the Victorian era when our modern take on the holiday was firmly established in both deed and literature.&#39;, &#39;Saturnalia was a week-long Roman festival held between the 17th and 23rd of December that honoured the agricultural god Saturn, nicely encompassing the winter solstice, another event of significance in the pagan calendar.&#39;, &#39;For the rich, fine clothes and jewellery were the norm; for the less well-off, nicer food than usual, a bundle of firewood or simple wooden toys like spinning tops and dolls were eagerly anticipated.&#39;, &#39;The tradition of giving gifts on the 1st January remained strong, and this included Elizabeth I of England herself, who regularly received jewels, extravagant dresses, and feather fans from her courtiers.&#39;, &#34;With no public roads, travelling by horse and carriage was slow and uncomfortable; nevertheless, the more intrepid could visit sights like Francis Drake&#39;s Golden Hind ship in London, which had made the first English circumnavigation of the globe (1577 to 1580).&#34;, &#39;The Victorians certainly ensured that such medieval elements as a Christmas morning church service, feasting, games, gifts, and pantomimes continued to enjoy their status as essential activities of the season.&#39;, &#39;The young fir tree was decorated with candles and small presents (toys, sweets, charms, and candied fruit) hung from its boughs that were destined to be distributed to the Christmas guests who might have their name tagged to their gift.&#39;, &#39;A more efficient postage system and the introduction of the Penny Black stamp in 1840 meant that correspondence increased and a tradition developed of sending friends and distant family a Christmas card, first introduced in England in 1843.&#39;, &#39;The jolly figure with the long white beard who visits homes on Christmas Eve to leave good children presents has his origins in the 4th-century Saint Nicholas, Bishop of Myra in Anatolia who delighted in distributing gifts, including sacks of gold.&#39;, &#39;Nowadays, electric lights may have replaced the candles on the tree, churches are not quite as busy as they used to be, the Yule log is now usually chocolate, and many of the cards have become electronic, but the traditions that have run through the centuries to celebrate Christmas Day continue each and every year to enchant and inspire as they have always done.&#39;]], &#39;textrank_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.044585987261146494, recall=0.13861386138613863, fmeasure=0.06746987951807229))]), [&#39;Several of the traditions today strongly associated with Christmas have a very long history indeed, even pre-dating the Christmas celebration itself.&#39;, &#39;Christmas itself was seriously threatened by the Puritans, those Christian extremists who preferred to fast on Christmas day.&#39;, &#39;The Victorians certainly ensured that such medieval elements as a Christmas morning church service, feasting, games, gifts, and pantomimes continued to enjoy their status as essential activities of the season.&#39;, &#39;Not the first royal to have a Christmas tree in England, nevertheless, from 1841, Prince Albert began a lasting tradition which soon spread from town squares to living rooms across the country, the idea being spread by popular illustrated magazines that revealed the private festivities of the royal family.&#39;, &#39;Presents were now given primarily on Christmas Day or Christmas Eve.&#39;, &#39;The greatest gift-giver at Christmas is, of course, Father Christmas.&#39;, &#34;Father Christmas was not only inspired by Saint Nicholas but he also incorporated elements of the &#39;Spirit of Christmas&#39; of folklore, which explains his more jovial and spirit-loving side, a trait children hopefully appeal to by leaving him out an alcoholic drink of some sort on Christmas Eve.&#34;, &#39;There was, too, a change in timing, some families continued to eat a Christmas lunch, perhaps a little later than a normal one, while others ate a Christmas dinner in the evening.&#39;, &#34;Dickens&#39; festive tale A Christmas Carol, with its story of the reformed miser Ebenezer Scrooge, has itself become a staple part of Christmas ever since its publication in 1843.&#34;, &#39;Nowadays, electric lights may have replaced the candles on the tree, churches are not quite as busy as they used to be, the Yule log is now usually chocolate, and many of the cards have become electronic, but the traditions that have run through the centuries to celebrate Christmas Day continue each and every year to enchant and inspire as they have always done.&#39;]], &#39;tfidf_summary&#39;: [dict_items([(&#39;rouge2&#39;, Score(precision=0.023305084745762712, recall=0.10891089108910891, fmeasure=0.03839441535776614))]), [&#39;That parties everywhere could get out of hand is attested by records of watchmen being paid to ensure property was not damaged over the 12-day holiday, particularly the big parties held on the eve of the 6th of January, known as Twelfth Night.&#39;, &#34;The poor enjoyed more modest entertainment like cards and dice, carols, playing musical instruments, board games, telling folktales, and enjoying traditional party games like permitting one person to be the &#39;king of the feast&#39; if they found a bean in the special bread or cake – everyone else then had to mimic the &#39;king&#39; (a role-reversal game that echoed Saturnalia&#39;s similar &#39;Lord of Misrule&#39;).&#34;, &#34;During the Elizabethan Era (1558-1603 CE) &#39;holy days&#39; continued to be the main source of public &#39;holidays&#39; – a term now being used for the first time – but there were also more secular activities establishing themselves as popular traditions.&#34;, &#34;With no public roads, travelling by horse and carriage was slow and uncomfortable; nevertheless, the more intrepid could visit sights like Francis Drake&#39;s Golden Hind ship in London, which had made the first English circumnavigation of the globe (1577 to 1580).&#34;, &#39;The next leap forward in how Christmas was celebrated came during the reign of Queen Victoria from 1837 to 1901, a period which witnessed some significant new traditions that have since become a lasting part of the holiday season.&#39;, &#39;Not the first royal to have a Christmas tree in England, nevertheless, from 1841, Prince Albert began a lasting tradition which soon spread from town squares to living rooms across the country, the idea being spread by popular illustrated magazines that revealed the private festivities of the royal family.&#39;, &#39;The young fir tree was decorated with candles and small presents (toys, sweets, charms, and candied fruit) hung from its boughs that were destined to be distributed to the Christmas guests who might have their name tagged to their gift.&#39;, &#39;The jolly figure with the long white beard who visits homes on Christmas Eve to leave good children presents has his origins in the 4th-century Saint Nicholas, Bishop of Myra in Anatolia who delighted in distributing gifts, including sacks of gold.&#39;, &#34;Father Christmas was not only inspired by Saint Nicholas but he also incorporated elements of the &#39;Spirit of Christmas&#39; of folklore, which explains his more jovial and spirit-loving side, a trait children hopefully appeal to by leaving him out an alcoholic drink of some sort on Christmas Eve.&#34;, &#39;Nowadays, electric lights may have replaced the candles on the tree, churches are not quite as busy as they used to be, the Yule log is now usually chocolate, and many of the cards have become electronic, but the traditions that have run through the centuries to celebrate Christmas Day continue each and every year to enchant and inspire as they have always done.&#39;]]} . Conclusion . According to our experiments, not any single one of models can be the best model for any article. Therefore, it is worthwhile to test a bunch of different summarizer algorithms to choose the best one for our application. .",
            "url": "https://liwehappy.github.io/metastability/jupyter/machine%20learning/tf-idf/text%20summarization/rouge/lsa/textrank/2021/12/10/Text-Summarization-For-WHEncyclopedia.html",
            "relUrl": "/jupyter/machine%20learning/tf-idf/text%20summarization/rouge/lsa/textrank/2021/12/10/Text-Summarization-For-WHEncyclopedia.html",
            "date": " • Dec 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Text analysis and agile sarcasm prediction",
            "content": "Tradiontional approach to tackle sarcasm detection . import re from IPython.display import display import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from nltk.tokenize import word_tokenize import nltk from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import confusion_matrix from sklearn.metrics import precision_score from sklearn.metrics import recall_score, f1_score from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold from sklearn.pipeline import Pipeline from sklearn.pipeline import make_pipeline from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator from nltk.tokenize import casual_tokenize from sklearn.linear_model import LogisticRegression from sklearn.svm import LinearSVC from sklearn.model_selection import cross_val_score from sklearn.model_selection import GridSearchCV from sklearn.ensemble import StackingClassifier . from google.colab import files # !pip install kaggle uploaded = files.upload() # Then move kaggle.json into the folder where the API expects to find it. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json Downloading news-headlines-dataset-for-sarcasm-detection.zip to /content 0% 0.00/3.30M [00:00&lt;?, ?B/s] 100% 3.30M/3.30M [00:00&lt;00:00, 54.7MB/s] . !unzip /content/news-headlines-dataset-for-sarcasm-detection.zip . Archive: /content/news-headlines-dataset-for-sarcasm-detection.zip inflating: Sarcasm_Headlines_Dataset.json inflating: Sarcasm_Headlines_Dataset_v2.json . df_sarcasm_1 = pd.read_json(&#39;Sarcasm_Headlines_Dataset.json&#39;, lines=True) df_sarcasm_2 = pd.read_json(&#39;Sarcasm_Headlines_Dataset_v2.json&#39;, lines=True) display(df_sarcasm_1.head()) print(&quot;===============&quot;) display(df_sarcasm_2.head()) . article_link headline is_sarcastic . 0 https://www.huffingtonpost.com/entry/versace-b... | former versace store clerk sues over secret &#39;b... | 0 | . 1 https://www.huffingtonpost.com/entry/roseanne-... | the &#39;roseanne&#39; revival catches up to our thorn... | 0 | . 2 https://local.theonion.com/mom-starting-to-fea... | mom starting to fear son&#39;s web series closest ... | 1 | . 3 https://politics.theonion.com/boehner-just-wan... | boehner just wants wife to listen, not come up... | 1 | . 4 https://www.huffingtonpost.com/entry/jk-rowlin... | j.k. rowling wishes snape happy birthday in th... | 0 | . =============== . is_sarcastic headline article_link . 0 1 | thirtysomething scientists unveil doomsday clo... | https://www.theonion.com/thirtysomething-scien... | . 1 0 | dem rep. totally nails why congress is falling... | https://www.huffingtonpost.com/entry/donna-edw... | . 2 0 | eat your veggies: 9 deliciously different recipes | https://www.huffingtonpost.com/entry/eat-your-... | . 3 1 | inclement weather prevents liar from getting t... | https://local.theonion.com/inclement-weather-p... | . 4 1 | mother comes pretty close to using word &#39;strea... | https://www.theonion.com/mother-comes-pretty-c... | . print(&quot;df1_shape:&quot;, df_sarcasm_1.shape) print(&quot;df2_shape:&quot;, df_sarcasm_2.shape) #Combine two dataframe into one df_sarcasm = pd.concat([df_sarcasm_1, df_sarcasm_2]).reset_index(drop=True) del df_sarcasm_1, df_sarcasm_2 df_sarcasm.drop([&#39;article_link&#39;], inplace=True, axis=1) print(&quot;df_shape:&quot;, df_sarcasm.shape) df_sarcasm.head() . df1_shape: (26709, 3) df2_shape: (28619, 3) df_shape: (55328, 2) . headline is_sarcastic . 0 former versace store clerk sues over secret &#39;b... | 0 | . 1 the &#39;roseanne&#39; revival catches up to our thorn... | 0 | . 2 mom starting to fear son&#39;s web series closest ... | 1 | . 3 boehner just wants wife to listen, not come up... | 1 | . 4 j.k. rowling wishes snape happy birthday in th... | 0 | . df_sarcasm.isna().sum() . headline 0 is_sarcastic 0 dtype: int64 . Basic text propertires between headlines and sarcasm in this dataset . Balance of target label . sns.set_style(&quot;dark&quot;) sns.countplot(data=df_sarcasm, x=&#39;is_sarcastic&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa66410e8d0&gt; . This dataset seems not so imbalanced to affect performance the chosen machine learning algorithms. . Sentence Length . df_sarcasm[&#39;word_counts&#39;] = df_sarcasm.headline.str.split().apply(len) fig, axs = plt.subplots(1, 2, figsize=(10, 5)) # axs[0].hist(word_counts.values, color=&#39;red&#39;, range=(2, 5)) sns.boxplot(data=df_sarcasm.loc[df_sarcasm.is_sarcastic == 1], x=&#39;word_counts&#39;, color=&#39;red&#39;, ax=axs[0]) # sns.boxplot(data=df_sarcasm.loc[(df_sarcasm.is_sarcastic == 1) &amp; (df_sarcasm.word_counts &lt; 50)], x=&#39;word_counts&#39;, color=&#39;red&#39;, ax=axs[0]) axs[0].set_title(&#39;Sarcastic text&#39;) # axs[1].hist(word_counts, color=&#39;green&#39;) sns.boxplot(data=df_sarcasm.loc[df_sarcasm.is_sarcastic == 0], x=&#39;word_counts&#39;, color=&#39;green&#39;,ax=axs[1]) axs[1].set_title(&#39;Non-Sarcastic text&#39;) df_sarcasm.drop(columns=[&#39;word_counts&#39;], inplace=True) plt.show() . If we remove the outlier(more than 150 words) in sarcastic text group, sarcastic and non-sarcastic texts actually share similiar word counts distribution . df_sarcasm[&#39;word_counts&#39;] = df_sarcasm.headline.str.split().apply(len) fig, axs = plt.subplots(1, 2, figsize=(10, 5)) sns.boxplot(data=df_sarcasm.loc[(df_sarcasm.is_sarcastic == 1) &amp; (df_sarcasm.word_counts &lt; 50)], x=&#39;word_counts&#39;, color=&#39;red&#39;, ax=axs[0]) axs[0].set_title(&#39;Sarcastic text&#39;) sns.boxplot(data=df_sarcasm.loc[df_sarcasm.is_sarcastic == 0], x=&#39;word_counts&#39;, color=&#39;green&#39;,ax=axs[1]) axs[1].set_title(&#39;Non-Sarcastic text&#39;) df_sarcasm.drop(columns=[&#39;word_counts&#39;], inplace=True) plt.show() . EDA . Chart below shows most frequent words in our corpus. . all_words = df_sarcasm.headline.str.split(expand=True).unstack().value_counts()[:30] sns.set(rc={&#39;figure.figsize&#39;:(20, 12)}) sns.barplot(all_words.index, all_words.values) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd941533b50&gt; . wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=&quot;white&quot;).generate(df_sarcasm.loc[df_sarcasm.is_sarcastic == 1].headline.str.cat(sep=&#39; &#39;)) plt.figure(figsize=(20, 10)) plt.imshow(wordcloud, interpolation=&quot;bilinear&quot;) plt.axis(&quot;off&quot;) plt.title(&#39;Common words from Sarcastics Headline&#39;) plt.show() . wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=&quot;white&quot;).generate(df_sarcasm.loc[df_sarcasm.is_sarcastic == 0].headline.str.cat(sep=&#39; &#39;)) plt.figure() plt.imshow(wordcloud, interpolation=&quot;bilinear&quot;) plt.axis(&quot;off&quot;) plt.title(&#39;Common words from Non-Sarcastics Headline&#39;) plt.show() . Baseline Models . X_train, X_test, y_train, y_test = train_test_split(df_sarcasm.headline, df_sarcasm.is_sarcastic, random_state=12020) . def cross_validation(pipeline, data, label): models = [] kf = KFold(n_splits=5) mean_score = [] for train_index, valid_index in kf.split(data): X_train, y_train = data.iloc[train_index], label.iloc[train_index] X_valid, y_valid = data.iloc[valid_index], label.iloc[valid_index] pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_valid) models.append(pipeline) # y_special = pipeline.predict(np.array([&quot;Well, if you want to see less of me, maybe we should go out again&quot;])) # print(&quot;y_special:&quot;, y_special) print(&#39;Precision: %.3f&#39; % precision_score(y_true=y_valid, y_pred=y_pred)) print(&#39;Recall: %.3f&#39; % recall_score(y_true=y_valid, y_pred=y_pred)) print(&#39;F1: %.3f&#39; % f1_score(y_true=y_valid, y_pred=y_pred)) score = accuracy_score(y_true=y_valid, y_pred=y_pred) print(&#39;Accuracy: %.3f&#39; % score) mean_score.append(score) # sns.heatmap(confusion_matrix(y_valid, y_pred), annot=True) # plt.show() print(&quot;================================&quot;) print(f&quot;Finished, average accuracy is {np.mean(np.array(mean_score))}&quot;) . def Testset_Performance(text_preprocess, classifier, cv=False): pipe = Pipeline([text_preprocess, classifier, ]) if cv == True: cross_validation(pipe, X_train, y_train) # set_trace() pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) score = accuracy_score(y_true=y_test, y_pred=y_pred) sns.heatmap(confusion_matrix(y_test, y_pred), annot=True) plt.show() print(&#39;Test Set Accuracy: %.3f&#39; % score) . pipe = Pipeline([(&#39;count_vect&#39;, CountVectorizer()), (&#39;clf&#39;, MultinomialNB())]) cross_validation(pipe, X_train, y_train) . Precision: 0.886 Recall: 0.868 F1: 0.877 Accuracy: 0.890 ================================ Precision: 0.888 Recall: 0.867 F1: 0.877 Accuracy: 0.891 ================================ Precision: 0.895 Recall: 0.864 F1: 0.879 Accuracy: 0.889 ================================ Precision: 0.892 Recall: 0.874 F1: 0.883 Accuracy: 0.892 ================================ Precision: 0.892 Recall: 0.861 F1: 0.876 Accuracy: 0.888 ================================ Finished, average accuracy is 0.8898689114815692 . pipe = Pipeline([(&#39;tfidf_vect&#39;, TfidfVectorizer()), (&#39;clf&#39;, MultinomialNB())]) cross_validation(pipe, X_train, y_train) . Precision: 0.907 Recall: 0.813 F1: 0.857 Accuracy: 0.877 ================================ Precision: 0.914 Recall: 0.800 F1: 0.853 Accuracy: 0.876 ================================ Precision: 0.920 Recall: 0.802 F1: 0.857 Accuracy: 0.875 ================================ Precision: 0.914 Recall: 0.812 F1: 0.860 Accuracy: 0.877 ================================ Precision: 0.915 Recall: 0.806 F1: 0.857 Accuracy: 0.876 ================================ Finished, average accuracy is 0.8762290071238079 . pipe = Pipeline([(&#39;count_vect&#39;, CountVectorizer()), (&#39;clf&#39;, LogisticRegression(max_iter=200))]) cross_validation(pipe, X_train, y_train) . Precision: 0.903 Recall: 0.893 F1: 0.898 Accuracy: 0.908 ================================ Precision: 0.903 Recall: 0.888 F1: 0.896 Accuracy: 0.906 ================================ Precision: 0.908 Recall: 0.889 F1: 0.899 Accuracy: 0.906 ================================ Precision: 0.900 Recall: 0.892 F1: 0.896 Accuracy: 0.904 ================================ Precision: 0.911 Recall: 0.883 F1: 0.897 Accuracy: 0.906 ================================ Finished, average accuracy is 0.9061836859427105 . pipe = Pipeline([(&#39;tfidf_vect&#39;, TfidfVectorizer()), (&#39;clf&#39;, LogisticRegression(max_iter=200))]) cross_validation(pipe, X_train, y_train) . Precision: 0.867 Recall: 0.873 F1: 0.870 Accuracy: 0.882 ================================ Precision: 0.866 Recall: 0.872 F1: 0.869 Accuracy: 0.881 ================================ Precision: 0.875 Recall: 0.870 F1: 0.872 Accuracy: 0.881 ================================ Precision: 0.867 Recall: 0.877 F1: 0.872 Accuracy: 0.880 ================================ Precision: 0.867 Recall: 0.863 F1: 0.865 Accuracy: 0.876 ================================ Finished, average accuracy is 0.8801329874262684 . pipe = Pipeline([(&#39;count_vect&#39;, CountVectorizer()), (&#39;clf&#39;, LinearSVC(random_state=0, tol=1e-5))]) cross_validation(pipe, X_train, y_train) . Precision: 0.923 Recall: 0.906 F1: 0.914 Accuracy: 0.923 ================================ Precision: 0.921 Recall: 0.909 F1: 0.915 Accuracy: 0.924 ================================ Precision: 0.925 Recall: 0.903 F1: 0.914 Accuracy: 0.921 ================================ Precision: 0.919 Recall: 0.912 F1: 0.915 Accuracy: 0.921 ================================ Precision: 0.928 Recall: 0.896 F1: 0.912 Accuracy: 0.920 ================================ Finished, average accuracy is 0.9217755601270003 . pipe = Pipeline([(&#39;tfidf_vect&#39;, TfidfVectorizer()), (&#39;clf&#39;, LinearSVC(random_state=0, tol=1e-5))]) cross_validation(pipe, X_train, y_train) . Precision: 0.913 Recall: 0.909 F1: 0.911 Accuracy: 0.920 ================================ Precision: 0.910 Recall: 0.907 F1: 0.909 Accuracy: 0.917 ================================ Precision: 0.916 Recall: 0.903 F1: 0.909 Accuracy: 0.916 ================================ Precision: 0.912 Recall: 0.911 F1: 0.912 Accuracy: 0.918 ================================ Precision: 0.918 Recall: 0.899 F1: 0.909 Accuracy: 0.917 ================================ Finished, average accuracy is 0.9175341723563732 . Refine Corpus . Improve model with N-grams and different tokenization . pipe = Pipeline([(&#39;count_vect&#39;, CountVectorizer(ngram_range=(1, 3), tokenizer=casual_tokenize)), (&#39;clf&#39;, MultinomialNB())]) cross_validation(pipe, X_train, y_train) . Precision: 0.953 Recall: 0.921 F1: 0.937 Accuracy: 0.944 ================================ Precision: 0.953 Recall: 0.922 F1: 0.937 Accuracy: 0.944 ================================ Precision: 0.959 Recall: 0.912 F1: 0.935 Accuracy: 0.941 ================================ Precision: 0.964 Recall: 0.921 F1: 0.942 Accuracy: 0.947 ================================ Precision: 0.962 Recall: 0.909 F1: 0.935 Accuracy: 0.941 ================================ Finished, average accuracy is 0.9435608180402053 . pipe = Pipeline([(&#39;count_vect&#39;, CountVectorizer(ngram_range=(1, 3), tokenizer=casual_tokenize)), (&#39;clf&#39;, LogisticRegression(max_iter=200))]) cross_validation(pipe, X_train, y_train) . Precision: 0.934 Recall: 0.944 F1: 0.939 Accuracy: 0.944 ================================ Precision: 0.939 Recall: 0.935 F1: 0.937 Accuracy: 0.943 ================================ Precision: 0.938 Recall: 0.937 F1: 0.937 Accuracy: 0.942 ================================ Precision: 0.940 Recall: 0.946 F1: 0.943 Accuracy: 0.947 ================================ Precision: 0.950 Recall: 0.929 F1: 0.939 Accuracy: 0.945 ================================ Finished, average accuracy is 0.9442114843274775 . Lemmatization . import spacy spacy.load(&#39;en&#39;) lemmatizer = spacy.lang.en.English() def my_tokenizer(doc): tokens = lemmatizer(doc) return ([token.lemma_ for token in tokens]) . custom_vec = CountVectorizer(tokenizer=my_tokenizer, ngram_range=(1, 3)) estimators = [(&#39;tokenizer&#39;, custom_vec), (&#39;clf&#39;, MultinomialNB())] pipe = Pipeline([estimators[0], estimators[1]]) cross_validation(pipe, X_train, y_train) . Precision: 0.951 Recall: 0.926 F1: 0.938 Accuracy: 0.945 ================================ Precision: 0.954 Recall: 0.915 F1: 0.934 Accuracy: 0.942 ================================ Precision: 0.959 Recall: 0.913 F1: 0.935 Accuracy: 0.941 ================================ Precision: 0.962 Recall: 0.922 F1: 0.941 Accuracy: 0.947 ================================ Precision: 0.961 Recall: 0.910 F1: 0.935 Accuracy: 0.941 ================================ Finished, average accuracy is 0.9431270076086973 . In conclusion, casual tokenization is slightly better than lemmatizatoin, so we will use casual tokenizer to emsemble our models. . Stacking Classifiers . Stacking method as many people have known are one of many powerful ways to reduce biases of prediction of each estimator. Here, we use stacking method to ensemble two estimators, specifically two predictions from two estimators &#39;MNB&#39; and &#39;Logi&#39; will act as inputs to train our last classifier(LinearSVC) to produce final prediction. . estimators = [ (&#39;MNB&#39;, make_pipeline(CountVectorizer(ngram_range=(1, 5), tokenizer=casual_tokenize), MultinomialNB())), (&#39;Logi&#39;, make_pipeline(CountVectorizer(ngram_range=(1, 3), tokenizer=casual_tokenize), LogisticRegression(max_iter=200))), # (&#39;svr&#39;, make_pipeline(CountVectorizer(ngram_range=(1, 3), tokenizer=casual_tokenize), # LinearSVC(random_state=0, tol=1e-5, max_iter=500))) ] clf = StackingClassifier( # estimators=estimators, final_estimator=LogisticRegression(max_iter=200)) estimators=estimators, final_estimator=LinearSVC(random_state=0, tol=1e-5, max_iter=500)) # clf.fit(X_train, y_train).score(X_test, y_test) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) score = accuracy_score(y_true=y_test, y_pred=y_pred) sns.heatmap(confusion_matrix(y_test, y_pred), annot=True) plt.show() print(score) . 0.9627674956622325 . In this notebook, utilizting this traditional NLP approach, we improve accuracy of sarcasm sentiments detection from 88.9% to 96.2%. .",
            "url": "https://liwehappy.github.io/metastability/jupyter/text%20analytics/machine%20learning/ensembling/2021/11/25/Sarcasm-Detection.html",
            "relUrl": "/jupyter/text%20analytics/machine%20learning/ensembling/2021/11/25/Sarcasm-Detection.html",
            "date": " • Nov 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Cyclists' Safety Guide in NYC - Part One",
            "content": "import pandas as pd from IPython.display import display, HTML import seaborn as sns import matplotlib.pyplot as plt import altair as alt import datapane as dp import altair as alt import numpy as np from datetime import date, time . Where is the deadliest place for cyclist in NYC?? And Why?? . Big picture of collision incidents on the streets. . All the datasets came from NYC Open datawebsite . df_collision_crashes = pd.read_csv(&#39;Motor_Vehicle_Collisions_-_Crashes.csv&#39;,low_memory=False) display(df_collision_crashes.head()) . CRASH DATE CRASH TIME BOROUGH ZIP CODE LATITUDE LONGITUDE LOCATION ON STREET NAME CROSS STREET NAME OFF STREET NAME ... CONTRIBUTING FACTOR VEHICLE 2 CONTRIBUTING FACTOR VEHICLE 3 CONTRIBUTING FACTOR VEHICLE 4 CONTRIBUTING FACTOR VEHICLE 5 COLLISION_ID VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2 VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5 . 0 04/14/2021 | 5:32 | NaN | NaN | NaN | NaN | NaN | BRONX WHITESTONE BRIDGE | NaN | NaN | ... | Unspecified | NaN | NaN | NaN | 4407480 | Sedan | Sedan | NaN | NaN | NaN | . 1 04/13/2021 | 21:35 | BROOKLYN | 11217 | 40.68358 | -73.97617 | (40.68358, -73.97617) | NaN | NaN | 620 ATLANTIC AVENUE | ... | NaN | NaN | NaN | NaN | 4407147 | Sedan | NaN | NaN | NaN | NaN | . 2 04/15/2021 | 16:15 | NaN | NaN | NaN | NaN | NaN | HUTCHINSON RIVER PARKWAY | NaN | NaN | ... | NaN | NaN | NaN | NaN | 4407665 | Station Wagon/Sport Utility Vehicle | NaN | NaN | NaN | NaN | . 3 04/13/2021 | 16:00 | BROOKLYN | 11222 | NaN | NaN | NaN | VANDERVORT AVENUE | ANTHONY STREET | NaN | ... | Unspecified | NaN | NaN | NaN | 4407811 | Sedan | NaN | NaN | NaN | NaN | . 4 04/12/2021 | 8:25 | NaN | NaN | 0.00000 | 0.00000 | (0.0, 0.0) | EDSON AVENUE | NaN | NaN | ... | Unspecified | NaN | NaN | NaN | 4406885 | Station Wagon/Sport Utility Vehicle | Sedan | NaN | NaN | NaN | . 5 rows × 29 columns . display(df_collision_crashes.shape) . (1818612, 29) . display(df_collision_crashes.info(show_counts=True)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1818612 entries, 0 to 1818611 Data columns (total 29 columns): # Column Non-Null Count Dtype -- -- 0 CRASH DATE 1818612 non-null object 1 CRASH TIME 1818612 non-null object 2 BOROUGH 1257468 non-null object 3 ZIP CODE 1257252 non-null object 4 LATITUDE 1605282 non-null float64 5 LONGITUDE 1605282 non-null float64 6 LOCATION 1605282 non-null object 7 ON STREET NAME 1448206 non-null object 8 CROSS STREET NAME 1168335 non-null object 9 OFF STREET NAME 278037 non-null object 10 NUMBER OF PERSONS INJURED 1818594 non-null float64 11 NUMBER OF PERSONS KILLED 1818581 non-null float64 12 NUMBER OF PEDESTRIANS INJURED 1818612 non-null int64 13 NUMBER OF PEDESTRIANS KILLED 1818612 non-null int64 14 NUMBER OF CYCLIST INJURED 1818612 non-null int64 15 NUMBER OF CYCLIST KILLED 1818612 non-null int64 16 NUMBER OF MOTORIST INJURED 1818612 non-null int64 17 NUMBER OF MOTORIST KILLED 1818612 non-null int64 18 CONTRIBUTING FACTOR VEHICLE 1 1813434 non-null object 19 CONTRIBUTING FACTOR VEHICLE 2 1556895 non-null object 20 CONTRIBUTING FACTOR VEHICLE 3 123965 non-null object 21 CONTRIBUTING FACTOR VEHICLE 4 27014 non-null object 22 CONTRIBUTING FACTOR VEHICLE 5 7134 non-null object 23 COLLISION_ID 1818612 non-null int64 24 VEHICLE TYPE CODE 1 1808784 non-null object 25 VEHICLE TYPE CODE 2 1508860 non-null object 26 VEHICLE TYPE CODE 3 120369 non-null object 27 VEHICLE TYPE CODE 4 26216 non-null object 28 VEHICLE TYPE CODE 5 6943 non-null object dtypes: float64(4), int64(7), object(18) memory usage: 402.4+ MB . None . There are no any na value in CRASH DATE . df_collision_crashes[&#39;CRASH DATE&#39;].isna().sum() . 0 . Let&#39;s transform CRASH DATA column into datetime format in order to easily manipulate data. . df_collision_crashes[&#39;CRASH DATE&#39;] = pd.to_datetime(df_collision_crashes[&#39;CRASH DATE&#39;]) df_collision_crashes.head() . CRASH DATE CRASH TIME BOROUGH ZIP CODE LATITUDE LONGITUDE LOCATION ON STREET NAME CROSS STREET NAME OFF STREET NAME ... CONTRIBUTING FACTOR VEHICLE 2 CONTRIBUTING FACTOR VEHICLE 3 CONTRIBUTING FACTOR VEHICLE 4 CONTRIBUTING FACTOR VEHICLE 5 COLLISION_ID VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2 VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5 . 0 2021-04-14 | 5:32 | NaN | NaN | NaN | NaN | NaN | BRONX WHITESTONE BRIDGE | NaN | NaN | ... | Unspecified | NaN | NaN | NaN | 4407480 | Sedan | Sedan | NaN | NaN | NaN | . 1 2021-04-13 | 21:35 | BROOKLYN | 11217 | 40.68358 | -73.97617 | (40.68358, -73.97617) | NaN | NaN | 620 ATLANTIC AVENUE | ... | NaN | NaN | NaN | NaN | 4407147 | Sedan | NaN | NaN | NaN | NaN | . 2 2021-04-15 | 16:15 | NaN | NaN | NaN | NaN | NaN | HUTCHINSON RIVER PARKWAY | NaN | NaN | ... | NaN | NaN | NaN | NaN | 4407665 | Station Wagon/Sport Utility Vehicle | NaN | NaN | NaN | NaN | . 3 2021-04-13 | 16:00 | BROOKLYN | 11222 | NaN | NaN | NaN | VANDERVORT AVENUE | ANTHONY STREET | NaN | ... | Unspecified | NaN | NaN | NaN | 4407811 | Sedan | NaN | NaN | NaN | NaN | . 4 2021-04-12 | 8:25 | NaN | NaN | 0.00000 | 0.00000 | (0.0, 0.0) | EDSON AVENUE | NaN | NaN | ... | Unspecified | NaN | NaN | NaN | 4406885 | Station Wagon/Sport Utility Vehicle | Sedan | NaN | NaN | NaN | . 5 rows × 29 columns . Observe overall crashes accidents over these years . Add &#39;year&#39; column . df_collision_crashes[&#39;year&#39;] = df_collision_crashes[&#39;CRASH DATE&#39;].dt.year df_collision_crashes.head() . CRASH DATE CRASH TIME BOROUGH ZIP CODE LATITUDE LONGITUDE LOCATION ON STREET NAME CROSS STREET NAME OFF STREET NAME ... CONTRIBUTING FACTOR VEHICLE 3 CONTRIBUTING FACTOR VEHICLE 4 CONTRIBUTING FACTOR VEHICLE 5 COLLISION_ID VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2 VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5 year . 0 2021-04-14 | 5:32 | NaN | NaN | NaN | NaN | NaN | BRONX WHITESTONE BRIDGE | NaN | NaN | ... | NaN | NaN | NaN | 4407480 | Sedan | Sedan | NaN | NaN | NaN | 2021 | . 1 2021-04-13 | 21:35 | BROOKLYN | 11217 | 40.68358 | -73.97617 | (40.68358, -73.97617) | NaN | NaN | 620 ATLANTIC AVENUE | ... | NaN | NaN | NaN | 4407147 | Sedan | NaN | NaN | NaN | NaN | 2021 | . 2 2021-04-15 | 16:15 | NaN | NaN | NaN | NaN | NaN | HUTCHINSON RIVER PARKWAY | NaN | NaN | ... | NaN | NaN | NaN | 4407665 | Station Wagon/Sport Utility Vehicle | NaN | NaN | NaN | NaN | 2021 | . 3 2021-04-13 | 16:00 | BROOKLYN | 11222 | NaN | NaN | NaN | VANDERVORT AVENUE | ANTHONY STREET | NaN | ... | NaN | NaN | NaN | 4407811 | Sedan | NaN | NaN | NaN | NaN | 2021 | . 4 2021-04-12 | 8:25 | NaN | NaN | 0.00000 | 0.00000 | (0.0, 0.0) | EDSON AVENUE | NaN | NaN | ... | NaN | NaN | NaN | 4406885 | Station Wagon/Sport Utility Vehicle | Sedan | NaN | NaN | NaN | 2021 | . 5 rows × 30 columns . . From the plot below, it seems like crash incidents increase over time, and there was a drastic surge between 2012 to 2013. Unsurprisingly, crash incidents suddenly decrease, it might be due to the COVID-19 pandemic. . ax = sns.countplot(data=df_collision_crashes, x=&#39;year&#39;) ax.set(ylabel=&#39;crash counts&#39;) . [Text(0, 0.5, &#39;crash counts&#39;)] . Delve into the crashes that have injuries/fatalities of cyclists . Read another datasets . #collapse_show df_bike_counts = pd.read_csv(&#39;Bicycle_Counts.csv&#39;,low_memory=False) display(df_bike_counts.head()) display(df_bike_counts.shape) df_bike_counters = pd.read_csv(&#39;Bicycle_Counters.csv&#39;,low_memory=False) display(df_bike_counters.head()) display(df_bike_counters.shape) . . id counts date status site . 0 0 | 41.0 | 08/31/2012 12:00:00 AM | 4.0 | 100005020 | . 1 1 | 52.0 | 08/31/2012 12:15:00 AM | 4.0 | 100005020 | . 2 2 | 38.0 | 08/31/2012 12:30:00 AM | 4.0 | 100005020 | . 3 3 | 36.0 | 08/31/2012 12:45:00 AM | 4.0 | 100005020 | . 4 4 | 40.0 | 08/31/2012 01:00:00 AM | 4.0 | 100005020 | . (3950246, 5) . id name latitude longitude domain site timezone interval counter . 0 0 | Manhattan Bridge 2012 Test Bike Counter | 40.699810 | -73.985890 | New York City DOT | 100005020 | (UTC-05:00) US/Eastern;DST | 15 | NaN | . 1 5 | Ed Koch Queensboro Bridge Shared Path | 40.751038 | -73.940820 | New York City DOT | 100009428 | (UTC-05:00) US/Eastern;DST | 15 | Y2H19111445 | . 2 10 | 1st Avenue - 26th St N - Interference testing | 40.738830 | -73.977165 | New York City DOT | 100010020 | (UTC-05:00) US/Eastern;DST | 15 | Y2H18044984 | . 3 14 | Manhattan Bridge Interference Calibration 2018... | 0.000000 | 0.000000 | New York City DOT | 100048744 | (UTC-05:00) US/Eastern;DST | 15 | Y2H13074109 | . 4 20 | Columbus Ave at 86th St. | 40.787745 | -73.975021 | New York City DOT | 100057320 | (UTC-05:00) US/Eastern;DST | 15 | Y2H18055356 | . (23, 9) . Given overall number of cyclists also increase, it makes sense that crash incidents increased as well. However, it is interesting to observer that the number of overall cyclists did not decrease proportionally between 2019 to 2020 after the breakout of COVID-19, it is a point worth another investigation. . df_bike_counts[&#39;date&#39;] = pd.to_datetime(df_bike_counts[&#39;date&#39;]) df_bike_counts[&#39;year&#39;] = df_bike_counts[&#39;date&#39;].dt.year df_bike_totalcounts = df_bike_counts.groupby(by=[&#39;year&#39;]).sum().reset_index()[[&#39;counts&#39;, &#39;year&#39;]] df_bike_totalcounts[&#39;year&#39;] = df_bike_totalcounts[&#39;year&#39;].map(str) ax = sns.barplot(data=df_bike_totalcounts, x=&#39;year&#39;, y=&#39;counts&#39;) ax.set(ylabel=&#39;Bike counts&#39;) . . ax = sns.barplot(data=df_bike_totalcounts, x=&#39;year&#39;, y=&#39;counts&#39;) ax.set(ylabel=&#39;Bike counts&#39;) . [Text(0, 0.5, &#39;Bike counts&#39;)] . Observe how many percentage of cyclists suffer injuries and fatalities over all of victims . df_bike_counts.head() . id counts date status site year . 0 0 | 41.0 | 2012-08-31 00:00:00 | 4.0 | 100005020 | 2012 | . 1 1 | 52.0 | 2012-08-31 00:15:00 | 4.0 | 100005020 | 2012 | . 2 2 | 38.0 | 2012-08-31 00:30:00 | 4.0 | 100005020 | 2012 | . 3 3 | 36.0 | 2012-08-31 00:45:00 | 4.0 | 100005020 | 2012 | . 4 4 | 40.0 | 2012-08-31 01:00:00 | 4.0 | 100005020 | 2012 | . casualties_plus_year_cols = [&#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;, &#39;NUMBER OF PERSONS INJURED&#39;, &#39;NUMBER OF PERSONS KILLED&#39;, &#39;NUMBER OF PEDESTRIANS INJURED&#39;, &#39;NUMBER OF PEDESTRIANS KILLED&#39;, &#39;NUMBER OF MOTORIST INJURED&#39;, &#39;NUMBER OF MOTORIST KILLED&#39; ] . df_casualties = df_collision_crashes.groupby(by=[df_collision_crashes.year]).sum()[casualties_plus_year_cols] df_casualties[&#39;year&#39;] = df_casualties.index.map(str) df_casualties = df_casualties.reset_index(drop=True) df_casualties[&#39;NUMBER of TOTAL CASUALTIES&#39;] = df_casualties[[&#39;NUMBER OF PERSONS INJURED&#39;, &#39;NUMBER OF PERSONS KILLED&#39;]].sum(axis=1) df_casualties[&#39;NUMBER of TOTAL CYCLISTS CASUALTIES&#39;] = df_casualties[[&#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;]].sum(axis=1) df_casualties[&#39;NUMBER of TOTAL PEDESTRIANS CASUALTIES&#39;] = df_casualties[[&#39;NUMBER OF PEDESTRIANS INJURED&#39;, &#39;NUMBER OF PEDESTRIANS KILLED&#39;]].sum(axis=1) df_casualties[&#39;NUMBER of TOTAL MOTORIST CASUALTIES&#39;] = df_casualties[[&#39;NUMBER OF MOTORIST INJURED&#39;, &#39;NUMBER OF MOTORIST KILLED&#39;]].sum(axis=1) # df_casualties.drop(columns=casualties_plus_year_cols[2:], inplace=True) df_casualties.head() . NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED NUMBER OF PERSONS INJURED NUMBER OF PERSONS KILLED NUMBER OF PEDESTRIANS INJURED NUMBER OF PEDESTRIANS KILLED NUMBER OF MOTORIST INJURED NUMBER OF MOTORIST KILLED year NUMBER of TOTAL CASUALTIES NUMBER of TOTAL CYCLISTS CASUALTIES NUMBER of TOTAL PEDESTRIANS CASUALTIES NUMBER of TOTAL MOTORIST CASUALTIES . 0 2210 | 6 | 27453.0 | 137.0 | 5906 | 72 | 19331 | 59 | 2012 | 27590.0 | 2216 | 5978 | 19390 | . 1 4075 | 11 | 55124.0 | 297.0 | 11988 | 176 | 39060 | 110 | 2013 | 55421.0 | 4086 | 12164 | 39170 | . 2 4000 | 20 | 51223.0 | 262.0 | 11036 | 133 | 36176 | 109 | 2014 | 51485.0 | 4020 | 11169 | 36285 | . 3 4281 | 15 | 51358.0 | 243.0 | 10084 | 133 | 36992 | 95 | 2015 | 51601.0 | 4296 | 10217 | 37087 | . 4 4975 | 18 | 60317.0 | 246.0 | 11090 | 149 | 44010 | 72 | 2016 | 60563.0 | 4993 | 11239 | 44082 | . df_bike_totalcounts.head() . counts year . 0 1611856.0 | 2012 | . 1 315312.0 | 2013 | . 2 8712098.0 | 2014 | . 3 13006008.0 | 2015 | . 4 15169726.0 | 2016 | . df_casualties = df_casualties.merge(df_bike_totalcounts, on=&#39;year&#39;, how=&#39;right&#39;) df_casualties . NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED NUMBER OF PERSONS INJURED NUMBER OF PERSONS KILLED NUMBER OF PEDESTRIANS INJURED NUMBER OF PEDESTRIANS KILLED NUMBER OF MOTORIST INJURED NUMBER OF MOTORIST KILLED year NUMBER of TOTAL CASUALTIES NUMBER of TOTAL CYCLISTS CASUALTIES NUMBER of TOTAL PEDESTRIANS CASUALTIES NUMBER of TOTAL MOTORIST CASUALTIES counts . 0 2210 | 6 | 27453.0 | 137.0 | 5906 | 72 | 19331 | 59 | 2012 | 27590.0 | 2216 | 5978 | 19390 | 1611856.0 | . 1 4075 | 11 | 55124.0 | 297.0 | 11988 | 176 | 39060 | 110 | 2013 | 55421.0 | 4086 | 12164 | 39170 | 315312.0 | . 2 4000 | 20 | 51223.0 | 262.0 | 11036 | 133 | 36176 | 109 | 2014 | 51485.0 | 4020 | 11169 | 36285 | 8712098.0 | . 3 4281 | 15 | 51358.0 | 243.0 | 10084 | 133 | 36992 | 95 | 2015 | 51601.0 | 4296 | 10217 | 37087 | 13006008.0 | . 4 4975 | 18 | 60317.0 | 246.0 | 11090 | 149 | 44010 | 72 | 2016 | 60563.0 | 4993 | 11239 | 44082 | 15169726.0 | . 5 4889 | 27 | 60657.0 | 256.0 | 11151 | 127 | 44616 | 107 | 2017 | 60913.0 | 4916 | 11278 | 44723 | 13751607.0 | . 6 4725 | 10 | 61940.0 | 230.0 | 11124 | 122 | 46068 | 98 | 2018 | 62170.0 | 4735 | 11246 | 46166 | 12095989.0 | . 7 4986 | 31 | 61389.0 | 244.0 | 10568 | 131 | 45834 | 82 | 2019 | 61633.0 | 5017 | 10699 | 45916 | 11776726.0 | . 8 5575 | 29 | 44603.0 | 268.0 | 6689 | 102 | 32339 | 137 | 2020 | 44871.0 | 5604 | 6791 | 32476 | 13026636.0 | . 9 3249 | 9 | 33931.0 | 185.0 | 4495 | 83 | 24922 | 85 | 2021 | 34116.0 | 3258 | 4578 | 25007 | 7147808.0 | . 2019 is clearly the deadliest year for cyclists. However, given growing number of injured cyclists, it is still not clear why there is drastic change of fatal accidents in both 2018 and 2019. And even though 2020 had less fatal incidents than 2019, more injuries actually arose in 2020. . df_collision_crashes.columns . Index([&#39;CRASH DATE&#39;, &#39;CRASH TIME&#39;, &#39;BOROUGH&#39;, &#39;ZIP CODE&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;LOCATION&#39;, &#39;ON STREET NAME&#39;, &#39;CROSS STREET NAME&#39;, &#39;OFF STREET NAME&#39;, &#39;NUMBER OF PERSONS INJURED&#39;, &#39;NUMBER OF PERSONS KILLED&#39;, &#39;NUMBER OF PEDESTRIANS INJURED&#39;, &#39;NUMBER OF PEDESTRIANS KILLED&#39;, &#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;, &#39;NUMBER OF MOTORIST INJURED&#39;, &#39;NUMBER OF MOTORIST KILLED&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 1&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 2&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 3&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 4&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 5&#39;, &#39;COLLISION_ID&#39;, &#39;VEHICLE TYPE CODE 1&#39;, &#39;VEHICLE TYPE CODE 2&#39;, &#39;VEHICLE TYPE CODE 3&#39;, &#39;VEHICLE TYPE CODE 4&#39;, &#39;VEHICLE TYPE CODE 5&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . f, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 8)) sns.barplot(data=df_casualties, x=&#39;year&#39;, y=&#39;NUMBER OF CYCLIST INJURED&#39;, ax=axes[0]) sns.barplot(data=df_casualties, x=&#39;year&#39;, y=&#39;NUMBER OF CYCLIST KILLED&#39;, ax=axes[1]) plt.show() # ax.set(ylabel=&#39;Bike counts&#39;) . The figure below show how many percentage of people are cyclists among all the injuries and fatalities. . from pdb import set_trace def annotate_percentage(axis, df): for i, p in enumerate(axis.patches): if i &gt; 9 and i &lt; 20: percentage = &#39;{:.1f}%&#39;.format(100 * df[&#39;NUMBER of TOTAL CYCLISTS CASUALTIES&#39;][i - 10] / df[&#39;NUMBER of TOTAL CASUALTIES&#39;][i - 10]) x = p.get_width() y = p.get_y() + p.get_height() - 0.2 axis.annotate(percentage, (x, y), size = 200) else: continue sns.set_theme(style=&quot;whitegrid&quot;) # Initialize the matplotlib figure f, ax = plt.subplots(figsize=(130, 80)) # Plot the total crashes sns.set_color_codes(&quot;pastel&quot;) sns.barplot(x=&quot;NUMBER of TOTAL CASUALTIES&quot;, y=&quot;year&quot;, data=df_casualties, label=&quot;Total Casualties&quot;, color=&quot;b&quot;) # Plot the crashes where cyclists were victims sns.set_color_codes(&quot;muted&quot;) sns.barplot(x=&quot;NUMBER of TOTAL CYCLISTS CASUALTIES&quot;, y=&quot;year&quot;, data=df_casualties, label=&quot;Total Cyclists Casualties&quot;, color=&quot;b&quot;) annotate_percentage(ax, df_casualties) # Add a legend and informative axis label ax.legend(ncol=2, loc=&quot;upper right&quot;, frameon=True, fontsize=&#39;120&#39;) ax.set(xlim=(0, 65000), ylabel=&quot;years&quot;, xlabel=&quot;Number of Casualties&quot;) ax.set_ylabel(&quot;years&quot;, fontsize=200) ax.set_xlabel(&quot;Number of Casualties&quot;, fontsize=200) ax.tick_params(axis=&#39;x&#39;, labelsize=80) ax.tick_params(axis=&#39;y&#39;, labelsize=80) sns.despine(left=True, bottom=True) . Comparison of change rate between bike counts in the city and overall injuries/fatalities of cyclists over these . In the plot below, we ignore data from 2012 and 2021 because this dataset is not yet completely collected at the time of data collection. . Conclusion: Casualties trend roughly follow the change of bike counts all around new york city. Essentially, the more new yorkers biking on the street more casualties of cyclists produced. And the most peculiar thing is that the number of cyclists casualties still rise even when the bike counts decrease in 2019. That is how the issue of cyclists safety in new york city get noticed by news coverage and city government even consider the idea of licensing of biking in the city. . plt.figure(figsize=(15, 10)) sns.set_theme(style=&quot;whitegrid&quot;) ax = sns.barplot(data=df_casualties[(df_casualties.year != &#39;2012&#39;) &amp; (df_casualties.year != &#39;2021&#39;)], x=&quot;year&quot;, y=&#39;counts&#39;, color=&#39;g&#39;) ax.set_ylabel(&#39;Bike Counts&#39;, color=&#39;g&#39;, fontsize=20) ax.set_xlabel(&#39;year&#39;, fontsize=20) ax2 = plt.twinx() sns.lineplot(data=df_casualties[(df_casualties.year != &#39;2012&#39;) &amp; (df_casualties.year != &#39;2021&#39;)], x=&quot;year&quot;, y=&#39;NUMBER of TOTAL CYCLISTS CASUALTIES&#39;, color=&#39;r&#39;, ax=ax2) ax2.set_ylabel(&#39;Number of Cyclists Casualties&#39;, color=&#39;r&#39;, fontsize=20) ax2.spines[&#39;right&#39;].set_color(&#39;r&#39;) ax2.spines[&#39;right&#39;].set_linewidth(3) ax2.spines[&#39;left&#39;].set_color(&#39;g&#39;) ax2.spines[&#39;left&#39;].set_linewidth(3) # sns.despine(left=True, bottom=True) . Breaking down which borough have highest casualties over these years . print(f&#39;There are roughly {(100 * df_collision_crashes.BOROUGH.isna().sum() / len(df_collision_crashes.BOROUGH)):.2f}% missing instances in BOUROUGH column&#39;) display(df_collision_crashes.groupby(by=[&#39;BOROUGH&#39;]).sum()[casualties_plus_year_cols].style.set_caption(&quot;Sum of Casualties by Bourough from 2012 to 2021&quot;).set_table_styles([{ &#39;selector&#39;: &#39;caption&#39;, &#39;props&#39;: [ (&#39;font-size&#39;, &#39;16px&#39;) ] }])) print(&#39;======================================================================&#39;) display(df_collision_crashes.groupby(by=[&#39;BOROUGH&#39;, &#39;year&#39;]).sum()[casualties_plus_year_cols].style.set_caption(&quot;Sum of Casualties by Bourough and Years(2012 to 2021)&quot;).set_table_styles([{ &#39;selector&#39;: &#39;caption&#39;, &#39;props&#39;: [ (&#39;font-size&#39;, &#39;16px&#39;) ] }])) . There are roughly 30.86% missing instances in BOUROUGH column . Sum of Casualties by Bourough from 2012 to 2021 NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED NUMBER OF PERSONS INJURED NUMBER OF PERSONS KILLED NUMBER OF PEDESTRIANS INJURED NUMBER OF PEDESTRIANS KILLED NUMBER OF MOTORIST INJURED NUMBER OF MOTORIST KILLED . BOROUGH . BRONX 3497 | 17 | 54317.000000 | 199.000000 | 12532 | 110 | 38054 | 71 | . BROOKLYN 13306 | 52 | 119906.000000 | 496.000000 | 25816 | 271 | 80371 | 167 | . MANHATTAN 9751 | 30 | 53915.000000 | 264.000000 | 17823 | 194 | 26218 | 39 | . QUEENS 6657 | 28 | 92284.000000 | 440.000000 | 17810 | 242 | 67495 | 169 | . STATEN ISLAND 393 | 3 | 13877.000000 | 78.000000 | 2030 | 33 | 11435 | 41 | . ====================================================================== . Sum of Casualties by Bourough and Years(2012 to 2021) NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED NUMBER OF PERSONS INJURED NUMBER OF PERSONS KILLED NUMBER OF PEDESTRIANS INJURED NUMBER OF PEDESTRIANS KILLED NUMBER OF MOTORIST INJURED NUMBER OF MOTORIST KILLED . BOROUGH year . BRONX 2012 155 | 1 | 2878.000000 | 17.000000 | 753 | 8 | 1970 | 8 | . 2013 295 | 1 | 6259.000000 | 32.000000 | 1672 | 25 | 4292 | 6 | . 2014 356 | 1 | 5886.000000 | 19.000000 | 1505 | 10 | 4025 | 8 | . 2015 363 | 3 | 5626.000000 | 26.000000 | 1372 | 11 | 3891 | 12 | . 2016 364 | 1 | 6050.000000 | 19.000000 | 1345 | 14 | 4298 | 5 | . 2017 325 | 1 | 5845.000000 | 17.000000 | 1354 | 10 | 4167 | 6 | . 2018 356 | 1 | 6288.000000 | 21.000000 | 1444 | 10 | 4482 | 9 | . 2019 392 | 0 | 6528.000000 | 11.000000 | 1410 | 9 | 4726 | 2 | . 2020 615 | 5 | 5214.000000 | 23.000000 | 987 | 10 | 3612 | 8 | . 2021 276 | 3 | 3743.000000 | 14.000000 | 690 | 3 | 2591 | 7 | . BROOKLYN 2012 879 | 2 | 7526.000000 | 33.000000 | 1836 | 19 | 4805 | 12 | . 2013 1539 | 4 | 15411.000000 | 67.000000 | 3641 | 30 | 10231 | 33 | . 2014 1409 | 4 | 14245.000000 | 67.000000 | 3348 | 42 | 9487 | 21 | . 2015 1513 | 4 | 13706.000000 | 67.000000 | 3028 | 46 | 9164 | 17 | . 2016 1452 | 6 | 13452.000000 | 37.000000 | 2917 | 20 | 9035 | 7 | . 2017 1300 | 7 | 12047.000000 | 40.000000 | 2663 | 20 | 8089 | 14 | . 2018 1368 | 2 | 13197.000000 | 39.000000 | 2785 | 23 | 9034 | 14 | . 2019 1397 | 13 | 13008.000000 | 59.000000 | 2700 | 33 | 8911 | 13 | . 2020 1553 | 10 | 9696.000000 | 50.000000 | 1780 | 17 | 6363 | 23 | . 2021 896 | 0 | 7618.000000 | 37.000000 | 1118 | 21 | 5252 | 13 | . MANHATTAN 2012 554 | 1 | 3868.000000 | 21.000000 | 1390 | 17 | 1924 | 3 | . 2013 1131 | 2 | 7585.000000 | 39.000000 | 2811 | 29 | 3643 | 8 | . 2014 1156 | 5 | 6909.000000 | 35.000000 | 2586 | 25 | 3167 | 5 | . 2015 1192 | 1 | 6531.000000 | 22.000000 | 2288 | 18 | 3051 | 3 | . 2016 1109 | 3 | 6236.000000 | 34.000000 | 2076 | 30 | 3083 | 0 | . 2017 974 | 8 | 5648.000000 | 33.000000 | 1770 | 21 | 2908 | 4 | . 2018 1005 | 2 | 5437.000000 | 19.000000 | 1784 | 12 | 2650 | 5 | . 2019 1032 | 2 | 5242.000000 | 24.000000 | 1565 | 20 | 2644 | 2 | . 2020 958 | 5 | 3574.000000 | 22.000000 | 880 | 10 | 1736 | 7 | . 2021 640 | 1 | 2885.000000 | 15.000000 | 673 | 12 | 1412 | 2 | . QUEENS 2012 402 | 2 | 5506.000000 | 28.000000 | 1172 | 15 | 3932 | 11 | . 2013 721 | 1 | 10686.000000 | 71.000000 | 2327 | 45 | 7638 | 25 | . 2014 714 | 6 | 10031.000000 | 52.000000 | 2192 | 26 | 7122 | 20 | . 2015 764 | 6 | 10423.000000 | 51.000000 | 2004 | 25 | 7655 | 20 | . 2016 691 | 1 | 10928.000000 | 31.000000 | 2080 | 19 | 8081 | 10 | . 2017 703 | 4 | 10103.000000 | 40.000000 | 1923 | 23 | 7471 | 13 | . 2018 625 | 1 | 10818.000000 | 53.000000 | 2016 | 31 | 8183 | 21 | . 2019 721 | 5 | 11095.000000 | 41.000000 | 2034 | 26 | 8340 | 10 | . 2020 858 | 1 | 7270.000000 | 46.000000 | 1288 | 20 | 5124 | 25 | . 2021 458 | 1 | 5424.000000 | 27.000000 | 774 | 12 | 3949 | 14 | . STATEN ISLAND 2012 27 | 0 | 1245.000000 | 11.000000 | 201 | 3 | 1017 | 8 | . 2013 35 | 0 | 2111.000000 | 4.000000 | 316 | 3 | 1760 | 1 | . 2014 39 | 0 | 1577.000000 | 10.000000 | 230 | 5 | 1308 | 5 | . 2015 30 | 1 | 1548.000000 | 12.000000 | 236 | 5 | 1282 | 6 | . 2016 46 | 1 | 1484.000000 | 9.000000 | 232 | 3 | 1192 | 5 | . 2017 49 | 0 | 1550.000000 | 8.000000 | 240 | 4 | 1260 | 4 | . 2018 39 | 0 | 1340.000000 | 7.000000 | 180 | 3 | 1121 | 4 | . 2019 40 | 1 | 1403.000000 | 4.000000 | 219 | 2 | 1144 | 1 | . 2020 49 | 0 | 935.000000 | 8.000000 | 124 | 3 | 762 | 5 | . 2021 39 | 0 | 684.000000 | 5.000000 | 52 | 2 | 589 | 2 | . df_cyclists_borough_yearly = df_collision_crashes.groupby(by=[&#39;BOROUGH&#39;, &#39;CRASH DATE&#39;]).sum()[casualties_plus_year_cols].reset_index()[[&#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;, &#39;CRASH DATE&#39;, &#39;BOROUGH&#39;]] df_cyclists_borough_yearly[&#39;Total Casualties of Cyclists&#39;] = df_cyclists_borough_yearly[[&#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;]].sum(axis=1) df_cyclists_borough_yearly.head() . NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED CRASH DATE BOROUGH Total Casualties of Cyclists . 0 2 | 0 | 2012-07-01 | BRONX | 2 | . 1 1 | 0 | 2012-07-02 | BRONX | 1 | . 2 2 | 0 | 2012-07-03 | BRONX | 2 | . 3 1 | 0 | 2012-07-04 | BRONX | 1 | . 4 0 | 0 | 2012-07-05 | BRONX | 0 | . Overall, BROOKLYN has the highest casualities . alt.data_transformers.disable_max_rows() plot1 = alt.Chart(df_cyclists_borough_yearly).mark_bar().encode( x=&#39;year(CRASH DATE):O&#39;, y=&#39;sum(Total Casualties of Cyclists):Q&#39;, color=&#39;BOROUGH:N&#39;, ).facet( facet=&#39;BOROUGH:N&#39;, columns=3 ) dp.Report( dp.Plot(plot1), ).upload(name=&quot;Plot1&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/M7bQOm3/plot1/embed/&quot; width=&quot;60%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported plot2 = alt.Chart(df_cyclists_borough_yearly).mark_bar().encode( x=&#39;year(CRASH DATE):O&#39;, y=&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;, color=&#39;BOROUGH:N&#39;, ).facet( facet=&#39;BOROUGH:N&#39;, columns=3 ) dp.Report( dp.Plot(plot2), ).upload(name=&quot;Plot2&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/q34OzGA/plot2/embed/&quot; width=&quot;60%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported In conclusion, Brooklyn take up a significant proportion of casualties both in injuries and deaths. All of boroughs slightly show peaks between 2014 to 2017 and 2019 to 2020. . Breaking down at which month/day/time casualties concentrate . death_cols = [&#39;CRASH DATE&#39;, &#39;CRASH TIME&#39;, &#39;BOROUGH&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;, &#39;LOCATION&#39;, &#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 1&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 2&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 3&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 4&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 5&#39;, &#39;COLLISION_ID&#39;, &#39;VEHICLE TYPE CODE 1&#39;, &#39;VEHICLE TYPE CODE 2&#39;, &#39;VEHICLE TYPE CODE 3&#39;, &#39;VEHICLE TYPE CODE 4&#39;, &#39;VEHICLE TYPE CODE 5&#39;, &#39;year&#39;, &#39;ZIP CODE&#39;] df_casualties_with_time = df_collision_crashes[df_collision_crashes[[&#39;NUMBER OF CYCLIST KILLED&#39;, &#39;NUMBER OF CYCLIST INJURED&#39;]].sum(axis=1) != 0][death_cols]#.reset_index(drop=True) df_casualties_with_time[&#39;CRASH DATE&#39;] = pd.to_datetime(df_casualties_with_time[[&#39;CRASH DATE&#39;, &#39;CRASH TIME&#39;]].apply(lambda x: &#39; &#39;.join(x.values.astype(str)), axis=&quot;columns&quot;)) df_casualties_with_time_location = df_casualties_with_time.copy() df_casualties_with_time.drop(columns=[&#39;CRASH TIME&#39;, &#39;LOCATION&#39;, &#39;LATITUDE&#39;, &#39;LONGITUDE&#39;], inplace=True) df_casualties_with_time.head() . CRASH DATE BOROUGH NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED CONTRIBUTING FACTOR VEHICLE 1 CONTRIBUTING FACTOR VEHICLE 2 CONTRIBUTING FACTOR VEHICLE 3 CONTRIBUTING FACTOR VEHICLE 4 CONTRIBUTING FACTOR VEHICLE 5 COLLISION_ID VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2 VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5 year ZIP CODE . 52 2021-04-16 11:00:00 | QUEENS | 1 | 0 | Turning Improperly | Unspecified | NaN | NaN | NaN | 4407792 | Station Wagon/Sport Utility Vehicle | Bike | NaN | NaN | NaN | 2021 | 11368 | . 90 2021-04-14 00:00:00 | NaN | 1 | 0 | Failure to Yield Right-of-Way | Unspecified | NaN | NaN | NaN | 4407649 | Station Wagon/Sport Utility Vehicle | Bike | NaN | NaN | NaN | 2021 | NaN | . 139 2021-04-13 17:55:00 | BRONX | 1 | 0 | Pedestrian/Bicyclist/Other Pedestrian Error/Co... | Unspecified | NaN | NaN | NaN | 4407789 | Station Wagon/Sport Utility Vehicle | Bike | NaN | NaN | NaN | 2021 | 10452 | . 145 2021-04-14 19:45:00 | BROOKLYN | 1 | 0 | Driver Inattention/Distraction | Driver Inattention/Distraction | NaN | NaN | NaN | 4407414 | Sedan | Bike | NaN | NaN | NaN | 2021 | 11201 | . 178 2021-04-16 00:30:00 | QUEENS | 1 | 0 | Unsafe Speed | Unspecified | Unspecified | NaN | NaN | 4407714 | Sedan | Bike | NaN | NaN | NaN | 2021 | 11369 | . Month versus Year distribution of Injured Cyclists . First, analyze the most dangerous borough to cycling, that is Brooklyn. . plot3 = alt.Chart(df_casualties_with_time).mark_circle().encode( alt.X(&#39;month(CRASH DATE):O&#39;), alt.Y(&#39;year(CRASH DATE):O&#39;), size=alt.Size(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;, legend=alt.Legend(title=&#39;Sum of Injured Cyclists&#39;)), color=&#39;BOROUGH:N&#39;, # column=&#39;BOROUGH:N&#39; facet=alt.Facet(&#39;BOROUGH:N&#39;, columns=2) ).properties( title=&#39;Month versus Year distribution of Injured Cyclists&#39;) dp.Report( dp.Plot(plot3), ).upload(name=&quot;Plot3&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/dA68E47/plot3/embed/&quot; width=&quot;60%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported Month versus Year distribution of Killed Cyclists . plot4 = alt.Chart(df_casualties_with_time).mark_circle().encode( alt.X(&#39;month(CRASH DATE):O&#39;), alt.Y(&#39;year(CRASH DATE):O&#39;), size=alt.Size(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;, legend=alt.Legend(title=&#39;Sum of Killed Cyclists&#39;)), color=&#39;BOROUGH:N&#39;, ).facet( facet=&#39;BOROUGH:N&#39;, columns=2, ).properties( title=&#39;Month versus Year distribution of Killed Cyclists&#39;) dp.Report( dp.Plot(plot4), ).upload(name=&quot;Plot4&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/vAqwOm7/plot4/embed/&quot; width=&quot;50%&quot; height=&quot;450px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported Time versus Day distribution of Injured Cyclists . plot5 = alt.Chart(df_casualties_with_time).mark_circle().encode( alt.X(&#39;hours(CRASH DATE):O&#39;), alt.Y(&#39;day(CRASH DATE):O&#39;), size=alt.Size(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;, legend=alt.Legend(title=&#39;Sum of Injured Cyclists&#39;)), color=&#39;BOROUGH:N&#39;, ).facet( facet=&#39;BOROUGH:N&#39;, columns=2, ).properties( title=&#39;Time versus Day distribution of Injured Cyclists&#39;) dp.Report( dp.Plot(plot5), ).upload(name=&quot;Plot5&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/9AxprGA/plot5/embed/&quot; width=&quot;80%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported Time versus Day distribution of Killed Cyclists . plot6 = alt.Chart(df_casualties_with_time).mark_circle().encode( alt.X(&#39;hours(CRASH DATE):O&#39;), alt.Y(&#39;day(CRASH DATE):O&#39;), size=alt.Size(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;, legend=alt.Legend(title=&#39;Sum of Killed Cyclists&#39;)), color=&#39;BOROUGH:N&#39;, ).facet( facet=&#39;BOROUGH:N&#39;, columns=2, ).properties( title=&#39;Time versus Day distribution of Killed Cyclists&#39;) dp.Report( dp.Plot(plot6), ).upload(name=&quot;Plot6&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/wAw2ylk/plot6/embed/&quot; width=&quot;80%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported Spatial distribution of cyclists-involved crashes . print(f&#39;Totoally, there are {(100 * df_casualties_with_time_location[~((df_casualties_with_time_location.LONGITUDE.between(-74.259, -73.59) &amp; (df_casualties_with_time_location.LATITUDE.between(40.49, 40.93))))].shape[0] / df_casualties_with_time_location.shape[0]):.2f} % rows of data are anomalous(null or unusual value) Location data&#39;) . Totoally, there are 7.69 % rows of data are anomalous(null or unusual value) Location data . We define location column to be False when LOGITUDE and LATITUDE information is not available. . df_casualties_with_time_location[&#39;location&#39;] = False # Limit all of location infos to legitimate range of New York City df_casualties_with_time_location.loc[(df_casualties_with_time_location.LONGITUDE.between(-74.259, -73.59) &amp; df_casualties_with_time_location.LATITUDE.between(40.49, 40.93)), &#39;location&#39;] = True df_casualties_with_time_location.location.value_counts() . True 39400 False 3280 Name: location, dtype: int64 . To quickly examine the distribution of these small part of unspecified LOCATION datasets, we plot 4 charts below with respect to time(year, month, day, hours)/Borough/casualties), and we observe: . In terms of time-related columns, these small parts of dataset did not look like to be skewing into any part of values. | For those datapoints which are null value in BOROUGH column, they incline to have unspecified LOCATION as well. | . base = alt.Chart(df_casualties_with_time_location) # xscale = alt.Scale(domain=(4.0, 8.0)) # yscale = alt.Scale(domain=(1.9, 4.55)) area_args = {&#39;opacity&#39;: .3, &#39;interpolate&#39;: &#39;step&#39;} points = base.mark_circle().encode( alt.X(&#39;year:O&#39;), alt.Y(&#39;hours(CRASH DATE):T&#39;), color=&#39;location&#39; ) top_hist = base.mark_area(**area_args).encode( alt.X(&#39;year:O&#39;, # when using bins, the axis scale is set through # the bin extent, so we do not specify the scale here # (which would be ignored anyway) stack=None, title=&#39;&#39; ), alt.Y(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(height=100) right_hist = base.mark_area(**area_args).encode( alt.Y(&#39;hours(CRASH DATE):T&#39;, stack=None, title=&#39;&#39;, ), alt.X(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(width=100) plot7 = (top_hist &amp; (points | right_hist)) dp.Report( dp.Plot(plot7), ).upload(name=&quot;Plot7&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/8AVqaEk/plot7/embed/&quot; width=&quot;50%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported base = alt.Chart(df_casualties_with_time_location) # xscale = alt.Scale(domain=(4.0, 8.0)) # yscale = alt.Scale(domain=(1.9, 4.55)) area_args = {&#39;opacity&#39;: .3, &#39;interpolate&#39;: &#39;step&#39;} points = base.mark_circle().encode( alt.X(&#39;month(CRASH DATE):O&#39;), alt.Y(&#39;day(CRASH DATE):O&#39;), color=&#39;location&#39; ) top_hist = base.mark_area(**area_args).encode( alt.X(&#39;month(CRASH DATE):O&#39;, # when using bins, the axis scale is set through # the bin extent, so we do not specify the scale here # (which would be ignored anyway) stack=None, title=&#39;&#39; ), alt.Y(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(height=100) right_hist = base.mark_area(**area_args).encode( alt.Y(&#39;day(CRASH DATE):O&#39;, stack=None, title=&#39;&#39;, ), alt.X(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(width=100) plot8 = (top_hist &amp; (points | right_hist)) dp.Report( dp.Plot(plot8), ).upload(name=&quot;Plot8&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/dA9b0O3/plot8/embed/&quot; width=&quot;50%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported base = alt.Chart(df_casualties_with_time_location) # xscale = alt.Scale(domain=(4.0, 8.0)) # yscale = alt.Scale(domain=(1.9, 4.55)) area_args = {&#39;opacity&#39;: .3, &#39;interpolate&#39;: &#39;step&#39;} points = base.mark_circle().encode( alt.X(&#39;NUMBER OF CYCLIST KILLED:N&#39;), alt.Y(&#39;BOROUGH:N&#39;), color=&#39;location&#39; ) top_hist = base.mark_area(**area_args).encode( alt.X(&#39;BOROUGH:N&#39;, # when using bins, the axis scale is set through # the bin extent, so we do not specify the scale here # (which would be ignored anyway) stack=None, title=&#39;&#39; ), alt.Y(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(height=100) right_hist = base.mark_area(**area_args).encode( alt.Y(&#39;NUMBER OF CYCLIST KILLED:N&#39;, stack=None, title=&#39;&#39;, ), alt.X(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(width=100) plot9 = (top_hist &amp; (points | right_hist)) dp.Report( dp.Plot(plot9), ).upload(name=&quot;Plot9&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/nkKzMy7/plot9/embed/&quot; width=&quot;50%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported base = alt.Chart(df_casualties_with_time_location) # xscale = alt.Scale(domain=(4.0, 8.0)) # yscale = alt.Scale(domain=(1.9, 4.55)) area_args = {&#39;opacity&#39;: .3, &#39;interpolate&#39;: &#39;step&#39;} points = base.mark_circle().encode( alt.X(&#39;NUMBER OF CYCLIST INJURED:O&#39;), alt.Y(&#39;BOROUGH:N&#39;), color=&#39;location&#39; ) top_hist = base.mark_area(**area_args).encode( alt.X(&#39;NUMBER OF CYCLIST INJURED:O&#39;, # when using bins, the axis scale is set through # the bin extent, so we do not specify the scale here # (which would be ignored anyway) stack=None, title=&#39;&#39; ), alt.Y(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(height=100) right_hist = base.mark_area(**area_args).encode( alt.Y(&#39;BOROUGH:N&#39;, stack=None, title=&#39;&#39;, ), alt.X(&#39;count()&#39;, stack=None, title=&#39;&#39;), alt.Color(&#39;location:N&#39;), ).properties(width=100) plot10 = (top_hist &amp; (points | right_hist)) dp.Report( dp.Plot(plot10), ).upload(name=&quot;Plot10&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/P3XKWMA/plot10/embed/&quot; width=&quot;50%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported df_casualties_with_time_location = df_casualties_with_time_location[(df_casualties_with_time_location.LONGITUDE.between(-74.259, -73.59) &amp; (df_casualties_with_time_location.LATITUDE.between(40.49, 40.93)))] . Here, we ignore 7.51 % of datapoints from those unspecified location datapoints in order to visualize spacial distribution of cyclists crashes in new york city below. . The plot below enable the adjustable visualzation of yearly crashes in five boroughs of NYC. Note: It is recommended to interact with plots below online by clicking buttom left button due to performace reason . boroughs = alt.topo_feature(&#39;https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&amp;format=GeoJSON&#39;, &#39;boroughs&#39;) background = alt.Chart(boroughs).mark_geoshape( fill=&#39;lightgray&#39;, stroke=&#39;white&#39;, strokeWidth=2 ).encode( color=alt.value(&#39;#eee&#39;), ).properties( width=1000, height=800 ) points = alt.Chart(df_casualties_with_time_location[df_casualties_with_time_location.location == True]).mark_circle( size=10, color=&#39;steelblue&#39; ).encode( longitude=&#39;LONGITUDE:Q&#39;, latitude=&#39;LATITUDE:Q&#39;, color=&#39;BOROUGH:N&#39;, tooltip=[&#39;CRASH TIME&#39;, &#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;, &#39;VEHICLE TYPE CODE 1&#39;, &#39;VEHICLE TYPE CODE 2&#39;, &#39;VEHICLE TYPE CODE 3&#39;, &#39;VEHICLE TYPE CODE 4&#39;, &#39;VEHICLE TYPE CODE 5&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 1&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 2&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 3&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 4&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 5&#39;] # A Slider filter year_slider = alt.binding_range(min=2012, max=2021, step=1) slider_selection = alt.selection_single(bind=year_slider, fields=[&#39;year&#39;], name=&quot;Yearly Crashes&quot;) filter_year = points.add_selection( slider_selection ).transform_filter( slider_selection ).properties(title=&quot;Slider Filtering&quot;) (background + filter_year)# &amp; bars . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/d7dXOZ7/plot11/embed/&quot; width=&quot;100%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported boroughs = alt.topo_feature(&#39;https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&amp;format=GeoJSON&#39;, &#39;boroughs&#39;) background = alt.Chart(boroughs).mark_geoshape( fill=&#39;lightgray&#39;, stroke=&#39;white&#39;, strokeWidth=2 ).encode( color=alt.value(&#39;#eee&#39;), ).properties( width=1000, height=800 ) points = alt.Chart(df_casualties_with_time_location[df_casualties_with_time_location.location == True]).mark_circle( size=20, color=&#39;steelblue&#39; ).encode( longitude=&#39;LONGITUDE:Q&#39;, latitude=&#39;LATITUDE:Q&#39;, color=&#39;BOROUGH:N&#39;, tooltip=[&#39;CRASH TIME&#39;, &#39;NUMBER OF CYCLIST INJURED&#39;, &#39;NUMBER OF CYCLIST KILLED&#39;, &#39;VEHICLE TYPE CODE 1&#39;, &#39;VEHICLE TYPE CODE 2&#39;, &#39;VEHICLE TYPE CODE 3&#39;, &#39;VEHICLE TYPE CODE 4&#39;, &#39;VEHICLE TYPE CODE 5&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 1&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 2&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 3&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 4&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 5&#39;] ).transform_filter( (alt.datum[&#39;NUMBER OF CYCLIST KILLED&#39;] != 0) ) # A Slider filter year_slider = alt.binding_range(min=2012, max=2021, step=1) slider_selection = alt.selection_single(bind=year_slider, fields=[&#39;year&#39;], name=&quot;Yearly Crashes&quot;) filter_year = points.add_selection( slider_selection ).transform_filter( slider_selection ).properties(title=&quot;Slider Filtering&quot;) plot12 = background + filter_year dp.Report( dp.Plot(plot12), ).upload(name=&quot;Plot12&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/MA1pPYk/plot12/embed/&quot; width=&quot;100%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported Vehicles that normally collide with Bicycles . df_casualties_with_time.groupby(by=[&#39;BOROUGH&#39;, &#39;CONTRIBUTING FACTOR VEHICLE 1&#39;, &#39;year&#39;]).sum().sort_values(by=[&#39;NUMBER OF CYCLIST INJURED&#39;], ascending=False).loc[&#39;BROOKLYN&#39;] . NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED COLLISION_ID . CONTRIBUTING FACTOR VEHICLE 1 year . Unspecified 2013 986 | 3 | 160370315 | . 2014 755 | 2 | 795123211 | . 2015 686 | 1 | 2224404703 | . 2012 558 | 1 | 95388300 | . Driver Inattention/Distraction 2020 462 | 2 | 1981703348 | . ... ... ... | ... | ... | . Traffic Control Device Improper/Non-Working 2019 1 | 0 | 4174052 | . 2018 1 | 0 | 3933370 | . Tire Failure/Inadequate 2021 1 | 0 | 4446831 | . 2019 1 | 0 | 4162201 | . Following Too Closely 2015 0 | 1 | 3257329 | . 316 rows × 3 columns . In these charts below, we can see what kinds of vehicles are responsible for injuries or deaths in most crash events. Noticeably, it also have shown that vehicles casuing more lethal occurences are not necessarily the same as dangerous(cause more injuries than death) ones. Based on different borough, more bulky or speedy vehicles are actually triggering more deaths. . Specifically, among these vehicles, Passenger Vehicle, Sedan, Station Wagon/SUV are main culprits causing lots of casualties. Additionally, crashes with bus and box truck give rise to higher death deaths. . df_casualties_with_time.head() . CRASH DATE BOROUGH NUMBER OF CYCLIST INJURED NUMBER OF CYCLIST KILLED CONTRIBUTING FACTOR VEHICLE 1 CONTRIBUTING FACTOR VEHICLE 2 CONTRIBUTING FACTOR VEHICLE 3 CONTRIBUTING FACTOR VEHICLE 4 CONTRIBUTING FACTOR VEHICLE 5 COLLISION_ID VEHICLE TYPE CODE 1 VEHICLE TYPE CODE 2 VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5 year ZIP CODE . 52 2021-04-16 11:00:00 | QUEENS | 1 | 0 | Turning Improperly | Unspecified | NaN | NaN | NaN | 4407792 | Station Wagon/Sport Utility Vehicle | Bike | NaN | NaN | NaN | 2021 | 11368 | . 90 2021-04-14 00:00:00 | NaN | 1 | 0 | Failure to Yield Right-of-Way | Unspecified | NaN | NaN | NaN | 4407649 | Station Wagon/Sport Utility Vehicle | Bike | NaN | NaN | NaN | 2021 | NaN | . 139 2021-04-13 17:55:00 | BRONX | 1 | 0 | Pedestrian/Bicyclist/Other Pedestrian Error/Co... | Unspecified | NaN | NaN | NaN | 4407789 | Station Wagon/Sport Utility Vehicle | Bike | NaN | NaN | NaN | 2021 | 10452 | . 145 2021-04-14 19:45:00 | BROOKLYN | 1 | 0 | Driver Inattention/Distraction | Driver Inattention/Distraction | NaN | NaN | NaN | 4407414 | Sedan | Bike | NaN | NaN | NaN | 2021 | 11201 | . 178 2021-04-16 00:30:00 | QUEENS | 1 | 0 | Unsafe Speed | Unspecified | Unspecified | NaN | NaN | 4407714 | Sedan | Bike | NaN | NaN | NaN | 2021 | 11369 | . brooklyn = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;VEHICLE TYPE CODE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;BROOKLYN&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Brookyn&#39;) bronx = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;VEHICLE TYPE CODE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;BRONX&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Bronx&#39;) manhattan = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;VEHICLE TYPE CODE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;MANHATTAN&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Manhattan&#39;) queens = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;VEHICLE TYPE CODE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;QUEENS&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Queens&#39;) staten = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;VEHICLE TYPE CODE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;, legend=alt.Legend(title=&#39;NUMBER OF CYCLIST KILLED&#39;)) ).transform_filter( (alt.datum.BOROUGH == &#39;STATEN ISLAND&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Statne Island&#39;) dp.Report( dp.Page( title=&#39;Brooklyn&#39;, blocks=[&quot;### Plot&quot;, brooklyn] ), dp.Page( title=&#39;Bronx&#39;, blocks=[&quot;### Plot&quot;, bronx] ), dp.Page( title=&#39;Manhattan&#39;, blocks=[&quot;### Plot&quot;, manhattan] ), dp.Page( title=&#39;Queens&#39;, blocks=[&quot;### Plot&quot;, queens] ), dp.Page( title=&#39;Staten&#39;, blocks=[&quot;### Plot&quot;, staten] ), ).upload(name=&quot;Plot13&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/Y3YVnM7/plot13/embed/&quot; width=&quot;100%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported We also investigate contributing factors in cylists involved collisions. Common factors include: . Driver Inattention/Distraction | Failure to Yield Right-of-Way | Pedestrian/Bicyclist/Other Pedestrian Error/Confusion Unfortunately, the majority of contributing factors from collisions data are Unspecified. | . brooklyn = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;CONTRIBUTING FACTOR VEHICLE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;BROOKLYN&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Brookyn&#39;) bronx = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;CONTRIBUTING FACTOR VEHICLE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;BRONX&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Bronx&#39;) manhattan = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;CONTRIBUTING FACTOR VEHICLE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;MANHATTAN&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Manhattan&#39;) queens = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;CONTRIBUTING FACTOR VEHICLE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;) ).transform_filter( (alt.datum.BOROUGH == &#39;QUEENS&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Queens&#39;) staten = alt.Chart( df_casualties_with_time, ).mark_bar().encode( x=alt.X(&#39;CONTRIBUTING FACTOR VEHICLE 1:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;sum(NUMBER OF CYCLIST INJURED):Q&#39;), color=alt.Color(&#39;sum(NUMBER OF CYCLIST KILLED):Q&#39;, legend=alt.Legend(title=&#39;NUMBER OF CYCLIST KILLED&#39;)) ).transform_filter( (alt.datum.BOROUGH == &#39;STATEN ISLAND&#39;)# &amp; (alt.datum.year &lt; 2019) ).properties( title=&#39;Statne Island&#39;) dp.Report( dp.Page( title=&#39;Brooklyn&#39;, blocks=[&quot;### Plot&quot;, brooklyn] ), dp.Page( title=&#39;Bronx&#39;, blocks=[&quot;### Plot&quot;, bronx] ), dp.Page( title=&#39;Manhattan&#39;, blocks=[&quot;### Plot&quot;, manhattan] ), dp.Page( title=&#39;Queens&#39;, blocks=[&quot;### Plot&quot;, queens] ), dp.Page( title=&#39;Staten&#39;, blocks=[&quot;### Plot&quot;, staten] ), ).upload(name=&quot;Plot14&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/E7ylGg3/plot14/embed/&quot; width=&quot;100%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported What factorts contribute to Brooklyn&#39;s uncontested role as the most lethal area to bike. . We currently have already known what sorts of vehicles and main contributing factors were held accountable for vehemently high injuries/fatalities. . There are many aspects to consider reasons why brooklyn has much higher injuries/fatalities. . street traffic violation record | street design(road type) | traffic sign design | etc.. | . In this branch, we focus on traffic violation record. . How traffic violation records differs from each boroughs . df_summons = pd.read_csv(&#39;NYPD_B_Summons__Historic_.csv&#39;) df_summons.head() . EVNT_KEY VIOLATION_DATE VIOLATION_TIME CHG_LAW_CD VIOLATION_CODE VEH_CATEGORY CITY_NM RPT_OWNING_CMD X_COORD_CD Y_COORD_CD Latitude Longitude Location Point . 0 212468848 | 04/29/2020 | 02:54:13 | VTL | 4021 | CAR/SUV | BRONX | 41 | 1011681 | 234918 | 40.811437 | -73.900907 | POINT (-73.90090718699997 40.81143709100007) | . 1 218070566 | 09/19/2020 | 13:45:12 | VTL | 1110AB | BIKE | MANHATTAN | 28 | 997130 | 231440 | 40.801924 | -73.953479 | POINT (-73.95347921299998 40.801924136000025) | . 2 212905075 | 05/11/2020 | 07:11:59 | VTL | 1225D | CAR/SUV | MANHATTAN | 24 | 993243 | 228137 | 40.792863 | -73.967523 | POINT (-73.96752338099998 40.79286310800006) | . 3 217338795 | 09/02/2020 | 07:53:00 | VTL | 1229C3A | TRUCK/BUS | BROOKLYN | 69 | 1012161 | 178176 | 40.655693 | -73.899409 | POINT (-73.89940857299996 40.65569287400007) | . 4 218463147 | 09/29/2020 | 08:10:22 | VTL | 1225C2A | CAR/SUV | QUEENS | 103 | 1041717 | 197008 | 40.707240 | -73.792727 | POINT (-73.79272672599996 40.707239816000026) | . In this dataset, we have only violation records in 2020, so we only explore this dataset in 2020 from the perspective of moving violation summons. . df_violatecode = pd.read_excel(&#39;violationcodes.xlsx&#39;) df_violatecode.head() . LAW_TITLE ADJ Code Law Code printed on ticket DESCRIPTION can be issued to a bicyclist? . 0 NYCTRR | 413 | 4-13 | OPERATE TRUCK OFF AUTHORIZED RTE - NYC | N | . 1 VTL | 416 | 416 | OPERATE VEHICLE WITH DEALER PLATES MORE THAN 5... | N | . 2 VTL | 601 | 601 | LEAVE SCENE OF INCIDENT - DOMESTIC ANIMAL STRU... | N | . 3 VTL | 1102 | 1102 | FAILED TO COMPLY WITH LAWFUL ORDER | N | . 4 VTL | 1117 | 1117 | FAILED TO STOP AT MALFUNCTIONING TRAFFIC LIGHT | N | . df_violatecode[&#39;ADJ Code&#39;] = df_violatecode[&#39;ADJ Code&#39;].astype(&#39;str&#39;) # Check if ADJ Code column has any int type value left print(len([c for c in df_violatecode[&#39;ADJ Code&#39;] if type(c) == int])) # Check if VIOLATION_CODE column in df_summons still are str dtypes print(len([c for c in df_summons if type(c) == int])) . 0 0 . viocode_dict = df_violatecode.set_index(&#39;ADJ Code&#39;).DESCRIPTION.to_dict() # Fix data value error viocode_dict[&#39;37524A&#39;] = viocode_dict[&#39;37524AB&#39;] viocode_dict[&#39;408E5&#39;] = viocode_dict[&#39;408E5 &#39;] del viocode_dict[&#39;408E5 &#39;] # Create a new column in df_summons with created dictionary. df_summons[&#39;DESCRIPTION&#39;] = df_summons.VIOLATION_CODE.map(viocode_dict) df_summons[&#39;can be issued to a bicyclist?&#39;] = df_summons.VIOLATION_CODE.map(df_violatecode.set_index(&#39;ADJ Code&#39;)[&#39;can be issued to a bicyclist?&#39;].to_dict()) display(df_summons.shape) . (510293, 15) . By and large, Brooklyn has highest violation record from both CAR/SUV and Truck/BUS which also account for annual death of cyclists in other four boroughs. PS: There is no such borough called New York, it is a problem from datasets. And those locations actually could came from anywhere in New York city. . plt.figure(figsize=(20, 10)) sns.set_theme(style=&quot;whitegrid&quot;) g = sns.countplot( data=df_summons, y=&#39;CITY_NM&#39;, hue=&#39;VEH_CATEGORY&#39;, ) . Interactive bar plots below lists top 20 traffic violation conducts from highest to lowest, viewer can also play around radio buttons to see how changes vary depends on your chosen conditions. In conlusion, although Queens had highest over speeding viloations, brooklyn on the other hand had higher number of records from a variety of violation conducts such as speeding/disobeyed traffic device/operate vehicle while using electronic device/failt to yield cyclists and pedastrian that can explain why the contributing factor Driver Inattention/Distraction of crashes in brooklyn dominate other factors and is significant all over other boroughs. . df_summons_top20_tickets = df_summons.loc[df_summons.VIOLATION_CODE.isin(df_summons.VIOLATION_CODE.value_counts()[:20].index)][[&#39;CITY_NM&#39;, &#39;DESCRIPTION&#39;]] df_summons_top20_tickets.head() . alt.data_transformers.enable(&#39;default&#39;) alt.data_transformers.disable_max_rows() bars = alt.Chart( df_summons_top20_tickets ).mark_bar().encode( x=alt.X(&#39;DESCRIPTION:N&#39;, sort=&#39;-y&#39;), y=alt.Y(&#39;count():Q&#39;), color=alt.Color(&#39;count():Q&#39;), tooltip=[&#39;DESCRIPTION&#39;] ) boroughs = [&#39;BRONX&#39;, &#39;MANHATTAN&#39;, &#39;BROOKLYN&#39;, &#39;QUEENS&#39;]#, &#39;STATEN ISLAND&#39;, &#39;NEW YORK&#39;] borough_radio = alt.binding_radio(options=boroughs) borough_select = alt.selection_single(fields=[&#39;CITY_NM&#39;], bind=borough_radio, name=&quot;Boroughs&quot;, init={&#39;CITY_NM&#39;: &#39;BRONX&#39;}) filter_borough = bars.add_selection( borough_select ).transform_filter( borough_select ).properties(title=&#39;Violation Code Bar change by Boroughs&#39;) dp.Report( dp.Plot(filter_borough), ).upload(name=&quot;Plot17&quot;) . HTML(&#39;&lt;iframe src=&quot;https://datapane.com/u/jason0/reports/87NGoVk/plot17/embed/&quot; width=&quot;100%&quot; height=&quot;540px&quot; style=&quot;border: none;&quot;&gt;IFrame not supported&lt;/iframe&gt;&#39;) . IFrame not supported From the perspective of moving violation from streets in NYC, it is not so hard to imagine that Brooklyn with such high numbers of violations consistently reflect its hostile environment for cyclists and pedastrians every year. Particularly, moving violations of bicycles was not higher than Manhattan and Queens, however, moving violations of CAR/SUVs in Brooklyn is the highest among boroughs. . Altogether we can safely say Brooklyn is a relatively dangerous place to ride a bike, and we also get a slight idea why brookln always pose potential threats to cyclists. With all that being said, in this analysis we still have not found other factors contribute to Brooklyn&#39;s high crashes rate such as street design, traffic sign maintaince, etc.. . It wil be intersting to continue dig into this topics, all the insights gotten from analysis would have a potential application for effective city planing. Please wait for future analysis. .",
            "url": "https://liwehappy.github.io/metastability/jupyter/altair/datda%20visualization/seaborn/matplotlib/2021/10/21/Safety-Analysis-for-Cyclists-Part-One.html",
            "relUrl": "/jupyter/altair/datda%20visualization/seaborn/matplotlib/2021/10/21/Safety-Analysis-for-Cyclists-Part-One.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://liwehappy.github.io/metastability/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://liwehappy.github.io/metastability/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am currently on a self-directed journey of learning machine learning and project building. My main interests are machine learning, natural language processing, and physics. . Background . Previously, I worked as software engineer at CAMEO Communication,Inc. and received a M.S. in plasma physics from National Cheng Kung University, Taiwan. . Reason behind building this wensite . With right attitude, discipline, and methods, I believe every data enthusiasts out there can learn data science and make impacts for our society without related education background. Thus, by building this platform, hopefully I can reach out to those who also wants to break into this field and facilitate their progress of learning. . Publication . YT Lin (2018) Enhancement of Selected Species in Nonthermal Atmospheric Pressure Plasma: Implications on Wound Healing Effects. IEEE Transactions on Plasma Science, 2018 DOI: 10.1109/TPS.2018.2867495 . .",
          "url": "https://liwehappy.github.io/metastability/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://liwehappy.github.io/metastability/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}